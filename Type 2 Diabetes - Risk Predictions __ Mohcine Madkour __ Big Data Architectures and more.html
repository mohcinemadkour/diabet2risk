<!DOCTYPE html>
<!-- saved from url=(0091)http://localhost:8000/posts/2019/07/Machine%20Learning,%20July%202019,%20Risk%20prediction/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Mohcine Madkour, Big Data Architectures and more">


        <title>Type 2 Diabetes - Risk Predictions // Mohcine Madkour // Big Data Architectures and more</title>


    <link rel="stylesheet" href="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/pure-min.css">
    <link rel="stylesheet" href="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/font-awesome.min.css">
    <link rel="stylesheet" href="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/pure.css">
    <link rel="stylesheet" href="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/pygments.css">

    <div class="fit-vids-style">­<style>               .fluid-width-video-wrapper {                 width: 100%;                              position: relative;                       padding: 0;                            }                                                                                   .fluid-width-video-wrapper iframe,        .fluid-width-video-wrapper object,        .fluid-width-video-wrapper embed {           position: absolute;                       top: 0;                                   left: 0;                                  width: 100%;                              height: 100%;                          }                                       </style></div><script src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/jquery.min.js"></script>
    <script src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
<script type="text/javascript" async="" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/embed.js"></script><link rel="prefetch" as="style" href="https://c.disquscdn.com/next/embed/styles/lounge.15d8f2a22cfa6b9f96345c682b01a08f.css"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/common.bundle.b9167d06dc7bd01b59d6d6332d6aafa1.js"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/lounge.bundle.3c28b73070ee14be97ecb734e8aca3b4.js"><link rel="prefetch" as="script" href="https://disqus.com/next/config.js"><script src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/alfalfalfa.0823c767a3bc925f628afd9bed26c958.js" async="" charset="UTF-8"></script></head>

<body style="">
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
            <hgroup>
                <a href="http://localhost:8000/author/mohcine-madkour.html" title="See posts by Mohcine Madkour">
                        <img class="avatar" alt="Mohcine Madkour" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/ae08847efc1a85b710f326eb8ee2e907.png">
                </a>
                <h2 class="article-info">Mohcine Madkour</h2>
                <small class="about-author"></small>
                <h5>Published</h5>
                <p>Fri 12 July 2019</p>
                <a href="http://localhost:8000/">←Home</a>
            </hgroup>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Type 2 Diabetes - Risk Predictions</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="http://localhost:8000/tag/machine-learning/">Machine Learning</a>
                                <a class="post-category" href="http://localhost:8000/tag/july-2019/">July 2019</a>
                                <a class="post-category" href="http://localhost:8000/tag/risk-prediction/">Risk prediction</a>
                        </p>
                </header>
            </section>
            <p>Type 2 diabetes is a chronic condition that affects the way the body metabolizes sugar (glucose). With type 2 diabetes, the body either resists the effects of insulin (a hormone that regulates the movement of sugar into cells) or it doesn't produce enough insulin to maintain normal glucose levels.</p>
<p>Type 2 diabetes occurs more commonly in middle-aged and elderly people. Uncontrolled it can cause all sorts of very bad things: infections, damaged kidneys, vision loss and blindness, amputations and many more. So, there is no question that type 2 diabetes needs to be taken seriously and treated. </p>
<p>Type 2 diabetes is usually diagnosed using the <strong>glycated hemoglobin (A1C)</strong> test. This blood test indicates the average blood sugar level for the past two to three months. Normal levels are below 5.7 percent, and a result between 5.7 and 6.4 percent is considered prediabetes. An A1C level of 6.5 percent or higher on two separate tests means you have diabetes.</p>
<p>People who have diabetes need this test regularly to see if their levels are staying within range and if they need to adjust their diabetes medicines.</p>
<p>To treat type 2 diabetes lifestyle changes are very effective, and the side effects of eating more healthfully and staying more active are positive ones. </p>
<p>In this project I will try to predict A1C levels: no-diabetes, pre-diabetes and diabetes. I will transform the dataset from a regression task (A1C) into a multi-class classification task (3 A1C levels).</p>
<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/A1c_normal_to_high_ranges.png"></p>
<h3>Diabetes Dataset</h3>
<p>These data are courtesy of Dr John Schorling, Department of Medicine, University of Virginia School of Medicine.</p>
<p>The data consist of 19 variables on 403 subjects from 1046 subjects who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. According to Dr John Hong, Diabetes Mellitus Type II (adult onset diabetes) is associated most strongly with obesity. The waist/hip ratio may be a predictor in diabetes and heart disease. Type 2 Diabetes is also associated with hypertension - they may both be part of <strong>Metabolic Syndrome</strong>.</p>
<blockquote>
<p><strong>Metabolic syndrome</strong> is a collection of risk factors that includes high blood pressure, high blood sugar, excess body fat around the waist, and abnormal cholesterol levels. The syndrome increases the chance of developing heart disease, stroke, and diabetes. Aside from a large waist circumference, most of the disorders associated with metabolic syndrome have no symptoms. Losing weight, exercise, and dietary changes can help prevent or reverse metabolic syndrome.</p>
<p>According to a national health survey, more than 1 in 5 Americans has metabolic syndrome. The number of people with metabolic syndrome increases with age, affecting more than 40% of people in their 60s and 70s.</p>
</blockquote>
<p>The 403 subjects were the ones who were actually screened for diabetes. Glycosolated hemoglobin (A1C) &gt; 7.0 is usually taken as a positive diagnosis of diabetes. </p>
<p>Data obtained from <a class="vglnk" href="http://biostat.mc.vanderbilt.edu/DataSets" rel="nofollow"><span>http</span><span>://</span><span>biostat</span><span>.</span><span>mc</span><span>.</span><span>vanderbilt</span><span>.</span><span>edu</span><span>/</span><span>DataSets</span></a>.</p>
<h4>Description of Features:</h4>
<p>There are 403 observations and 19 features in this dataset, maximum # NaNs:262.</p>
<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/anonymous2x.png"></p>
<h3>Challenges</h3>
<p>We were facing two challenegs with my dataset:</p>
<ul>
<li>relatively small number of observations </li>
<li>imbalanced classes (A1C levels)</li>
</ul>
<p>To overcome the issues with imbalanced data, I will use several techniques:</p>
<ol>
<li>f1 macro averaged score for performance metric</li>
<li>cost-sensitive learning (penalize algorithms)</li>
<li>SMOTE - Synthetic Minority Over-sampling Technique</li>
</ol>
<p>and several machine learning algorithms:</p>
<ol>
<li>$L_1$-regularized Logistic Regression</li>
<li>$L_2$-regularized Logistic Regression</li>
<li>Support Vector Machine (SVM)</li>
<li>Random Forest</li>
<li>Gradient Boosting</li>
<li>AdaBoost</li>
</ol>
<p>All together, I have trained 22 models.</p>
<h3>Findings</h3>
<ul>
<li>From my limited sample I could not find any single condition that would alone increase the risk for type 2 diabetes.</li>
<li>We found that several factors could impact risks for the person to be diagnosed with diabetes: age, high cholesterol ratio, high blood presure, increased weight... </li>
<li>Even if you have all these conditions it does not mean you will have type 2 diabetes. This will make very difficult for my models to predict A1C levels.</li>
<li>Due to imbalanced data, all models had problems with predicting minority classes: <code>pre_diabetes</code> and <code>diabetes</code>. They were mostly predicting the majority class, <code>no_diabetes</code>.</li>
<li>At the end, I found that Random Forest algorithm with cost_sensitive learning did the best with f1 macro score of 0.56.</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># Pandas for DataFrames</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">'display.max_columns'</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Numpy for numerical computing</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># Matplotlib for visualization</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1"># display plots in the notebook</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Seaborn for easier visualization</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">'darkgrid'</span><span class="p">)</span>

<span class="c1"># display Python object in all frontends</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="c1"># store elements as dictionary keys and their counts as dictionary values</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Import Logistic Regression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Import SVM classifier </span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span> 

<span class="c1"># Import RandomForestClassifier and GradientBoostingClassifer</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span>

<span class="c1"># Function for splitting training and test set</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">StratifiedKFold</span>

<span class="c1"># Function for creating model pipelines - sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1"># Function for creating model pipelines - imblearn</span>
<span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span> <span class="k">as</span> <span class="n">imbl_pipe</span>

<span class="c1"># Over-sampling using SMOTE</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>

<span class="c1"># StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Classification metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">f1_score</span>

<span class="c1"># set class weights for imbalaced datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.class_weight</span> <span class="kn">import</span> <span class="n">compute_class_weight</span>

<span class="c1"># Ignore some warning messages</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="kn">import</span> <span class="n">DataConversionWarning</span><span class="p">,</span> <span class="n">ConvergenceWarning</span><span class="p">,</span> <span class="n">UndefinedMetricWarning</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">ConvergenceWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">DataConversionWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">UndefinedMetricWarning</span><span class="p">)</span>
</pre></div>


<h2>Exploratory Analysis</h2>
<p><strong>Importing the dataset</strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'Data/diabetes.csv'</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>chol</th>
      <th>stab.glu</th>
      <th>hdl</th>
      <th>ratio</th>
      <th>glyhb</th>
      <th>location</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>bp.1s</th>
      <th>bp.1d</th>
      <th>bp.2s</th>
      <th>bp.2d</th>
      <th>waist</th>
      <th>hip</th>
      <th>time.ppn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1000</td>
      <td>203.0</td>
      <td>82</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>Buckingham</td>
      <td>46</td>
      <td>female</td>
      <td>62.0</td>
      <td>121.0</td>
      <td>medium</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>38.0</td>
      <td>720.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1001</td>
      <td>165.0</td>
      <td>97</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>Buckingham</td>
      <td>29</td>
      <td>female</td>
      <td>64.0</td>
      <td>218.0</td>
      <td>large</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>46.0</td>
      <td>48.0</td>
      <td>360.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1002</td>
      <td>228.0</td>
      <td>92</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>4.64</td>
      <td>Buckingham</td>
      <td>58</td>
      <td>female</td>
      <td>61.0</td>
      <td>256.0</td>
      <td>large</td>
      <td>190.0</td>
      <td>92.0</td>
      <td>185.0</td>
      <td>92.0</td>
      <td>49.0</td>
      <td>57.0</td>
      <td>180.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1003</td>
      <td>78.0</td>
      <td>93</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>4.63</td>
      <td>Buckingham</td>
      <td>67</td>
      <td>male</td>
      <td>67.0</td>
      <td>119.0</td>
      <td>large</td>
      <td>110.0</td>
      <td>50.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>33.0</td>
      <td>38.0</td>
      <td>480.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1005</td>
      <td>249.0</td>
      <td>90</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>7.72</td>
      <td>Buckingham</td>
      <td>64</td>
      <td>male</td>
      <td>68.0</td>
      <td>183.0</td>
      <td>medium</td>
      <td>138.0</td>
      <td>80.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>44.0</td>
      <td>41.0</td>
      <td>300.0</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Renaming Feature Names</strong></p>
<p>Before I continue with EDA I will rename several features by replacing dots in the names with underscores to allow us using the "dot notation":</p>
<ul>
<li>
<p>stab.glu → stab_glu</p>
</li>
<li>
<p>bp.1s → bp_1s</p>
</li>
<li>
<p>bp.1d → bp_1d</p>
</li>
<li>
<p>bp.2s → bp_2s</p>
</li>
<li>
<p>bp.2d → bp_2d</p>
</li>
<li>
<p>time.ppn → time_ppn</p>
</li>
</ul>
<p>At the same time I will rename the target variable:</p>
<ul>
<li>glyhb → a1c</li>
</ul>
<p>The <em>ratio</em> feature is the cholesterol ratio: $chol \div hdl$. Lower the ratio the better. Let's rename this feature as well:</p>
<ul>
<li>ratio → chol_ratio</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># Rename feature names</span>
<span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">'glyhb'</span><span class="p">:</span><span class="s1">'a1c'</span><span class="p">,</span> <span class="s1">'stab.glu'</span><span class="p">:</span><span class="s1">'stab_glu'</span><span class="p">,</span> <span class="s1">'ratio'</span><span class="p">:</span><span class="s1">'chol_ratio'</span><span class="p">,</span><span class="s1">'bp.1s'</span><span class="p">:</span><span class="s1">'bp_1s'</span><span class="p">,</span> <span class="s1">'bp.1d'</span><span class="p">:</span><span class="s1">'bp_1d'</span><span class="p">,</span>
                   <span class="s1">'bp.2s'</span><span class="p">:</span><span class="s1">'bp_2s'</span><span class="p">,</span> <span class="s1">'bp.2d'</span><span class="p">:</span><span class="s1">'bp_2d'</span><span class="p">,</span> <span class="s1">'time.ppn'</span><span class="p">:</span><span class="s1">'time_ppn'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>chol</th>
      <th>stab_glu</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>location</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>bp_1s</th>
      <th>bp_1d</th>
      <th>bp_2s</th>
      <th>bp_2d</th>
      <th>waist</th>
      <th>hip</th>
      <th>time_ppn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1000</td>
      <td>203.0</td>
      <td>82</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>Buckingham</td>
      <td>46</td>
      <td>female</td>
      <td>62.0</td>
      <td>121.0</td>
      <td>medium</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>38.0</td>
      <td>720.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1001</td>
      <td>165.0</td>
      <td>97</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>Buckingham</td>
      <td>29</td>
      <td>female</td>
      <td>64.0</td>
      <td>218.0</td>
      <td>large</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>46.0</td>
      <td>48.0</td>
      <td>360.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1002</td>
      <td>228.0</td>
      <td>92</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>4.64</td>
      <td>Buckingham</td>
      <td>58</td>
      <td>female</td>
      <td>61.0</td>
      <td>256.0</td>
      <td>large</td>
      <td>190.0</td>
      <td>92.0</td>
      <td>185.0</td>
      <td>92.0</td>
      <td>49.0</td>
      <td>57.0</td>
      <td>180.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1003</td>
      <td>78.0</td>
      <td>93</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>4.63</td>
      <td>Buckingham</td>
      <td>67</td>
      <td>male</td>
      <td>67.0</td>
      <td>119.0</td>
      <td>large</td>
      <td>110.0</td>
      <td>50.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>33.0</td>
      <td>38.0</td>
      <td>480.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1005</td>
      <td>249.0</td>
      <td>90</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>7.72</td>
      <td>Buckingham</td>
      <td>64</td>
      <td>male</td>
      <td>68.0</td>
      <td>183.0</td>
      <td>medium</td>
      <td>138.0</td>
      <td>80.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>44.0</td>
      <td>41.0</td>
      <td>300.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="c1"># List names of the columns</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span>
</pre></div>


<div class="highlight"><pre><span></span>Index(['id', 'chol', 'stab_glu', 'hdl', 'chol_ratio', 'a1c', 'location', 'age',
       'gender', 'height', 'weight', 'frame', 'bp_1s', 'bp_1d', 'bp_2s',
       'bp_2d', 'waist', 'hip', 'time_ppn'],
      dtype='object')
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Dataframe dimensions</span>
<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(403, 19)
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Column datatypes</span>
<span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
</pre></div>


<div class="highlight"><pre><span></span>id              int64
chol          float64
stab_glu        int64
hdl           float64
chol_ratio    float64
a1c           float64
location       object
age             int64
gender         object
height        float64
weight        float64
frame          object
bp_1s         float64
bp_1d         float64
bp_2s         float64
bp_2d         float64
waist         float64
hip           float64
time_ppn      float64
dtype: object
</pre></div>


<ul>
<li>
<p>The feature names are now looking good.</p>
</li>
<li>
<p>As I expected, there are 403 observations and 19 features in the data set.</p>
</li>
<li>
<p>There are 16 numerical features and 3 categorical features. All data types are correct</p>
</li>
</ul>
<p><strong>Unused Features</strong></p>
<p>To make dataframe easily readable I will remove unused features like <code style="color:steelblue">id</code> and <code style="color:steelblue">location</code>.</p>
<p>The goal of this project is to predict ranges of A1C. From a quick look through my dataframe I could see that the postprandial time when labs were drawn varies. So, let's check its distribution.</p>
<div class="highlight"><pre><span></span><span class="c1"># Postprandial time in hours</span>
<span class="n">df_ppn_h</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'time_ppn'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">60</span>

<span class="c1"># Display min and max postprandial times</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Min. Postprandial Time:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">df_ppn_h</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span><span class="mi">2</span><span class="p">),</span> <span class="s1">'hours'</span><span class="p">,</span> <span class="s1">'('</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">time_ppn</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="s1">'minutes)'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Max. Postprandial Time:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">df_ppn_h</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span><span class="mi">2</span><span class="p">),</span> <span class="s1">'hours'</span><span class="p">,</span> <span class="s1">'('</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">time_ppn</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="s1">'minutes)'</span><span class="p">)</span>

<span class="c1"># Histogram for time_ppn in hours</span>
<span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'time_ppn'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>Min. Postprandial Time: 0.08 hours ( 5.0 minutes)
Max. Postprandial Time: 26.0 hours ( 1560.0 minutes)
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/_output_24_1.png"></p>
<p>Since the postprandial time when labs were drawn has very wide range, from 5 minutes to 26 hours, I could remove <code style="color:steelblue">stab_glu</code> and <code style="color:steelblue">time_ppn</code> features.</p>
<div class="highlight"><pre><span></span><span class="c1"># Drop unused features</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'id'</span><span class="p">,</span> <span class="s1">'stab_glu'</span><span class="p">,</span> <span class="s1">'location'</span><span class="p">,</span> <span class="s1">'time_ppn'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>bp_1s</th>
      <th>bp_1d</th>
      <th>bp_2s</th>
      <th>bp_2d</th>
      <th>waist</th>
      <th>hip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>62.0</td>
      <td>121.0</td>
      <td>medium</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>38.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>64.0</td>
      <td>218.0</td>
      <td>large</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>46.0</td>
      <td>48.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>4.64</td>
      <td>58</td>
      <td>female</td>
      <td>61.0</td>
      <td>256.0</td>
      <td>large</td>
      <td>190.0</td>
      <td>92.0</td>
      <td>185.0</td>
      <td>92.0</td>
      <td>49.0</td>
      <td>57.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>78.0</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>4.63</td>
      <td>67</td>
      <td>male</td>
      <td>67.0</td>
      <td>119.0</td>
      <td>large</td>
      <td>110.0</td>
      <td>50.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>33.0</td>
      <td>38.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>249.0</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>7.72</td>
      <td>64</td>
      <td>male</td>
      <td>68.0</td>
      <td>183.0</td>
      <td>medium</td>
      <td>138.0</td>
      <td>80.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>44.0</td>
      <td>41.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="c1"># Dataframe dimensions</span>
<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(403, 15)
</pre></div>


<p>The number of features has been decreased to 15.</p>
<h3>Distributions of Numeric Features</h3>
<p><strong>Plotting the histogram grid</strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Plot histogram grid</span>
<span class="n">df</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_31_0.png"></p>
<p>All distributions look OK. Only for <code style="color:steelblue">a1c</code> I could see many observations around 5% (healthy range). This might be an indication of imbalanced data classes once I transfer <code style="color:steelblue">a1c</code> to 3 classes.</p>
<p><strong>Summary statistics for the numeric features</strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Summarize numerical features</span>
<span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>height</th>
      <th>weight</th>
      <th>bp_1s</th>
      <th>bp_1d</th>
      <th>bp_2s</th>
      <th>bp_2d</th>
      <th>waist</th>
      <th>hip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>402.000000</td>
      <td>402.000000</td>
      <td>402.000000</td>
      <td>390.000000</td>
      <td>403.000000</td>
      <td>398.000000</td>
      <td>402.000000</td>
      <td>398.000000</td>
      <td>398.000000</td>
      <td>141.000000</td>
      <td>141.000000</td>
      <td>401.000000</td>
      <td>401.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>207.845771</td>
      <td>50.445274</td>
      <td>4.521642</td>
      <td>5.589769</td>
      <td>46.851117</td>
      <td>66.020101</td>
      <td>177.592040</td>
      <td>136.904523</td>
      <td>83.321608</td>
      <td>152.382979</td>
      <td>92.524823</td>
      <td>37.900249</td>
      <td>43.039900</td>
    </tr>
    <tr>
      <th>std</th>
      <td>44.445557</td>
      <td>17.262626</td>
      <td>1.727886</td>
      <td>2.242595</td>
      <td>16.312333</td>
      <td>3.918515</td>
      <td>40.340666</td>
      <td>22.741033</td>
      <td>13.589227</td>
      <td>21.712952</td>
      <td>11.555198</td>
      <td>5.729313</td>
      <td>5.656713</td>
    </tr>
    <tr>
      <th>min</th>
      <td>78.000000</td>
      <td>12.000000</td>
      <td>1.500000</td>
      <td>2.680000</td>
      <td>19.000000</td>
      <td>52.000000</td>
      <td>99.000000</td>
      <td>90.000000</td>
      <td>48.000000</td>
      <td>110.000000</td>
      <td>60.000000</td>
      <td>26.000000</td>
      <td>30.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>179.000000</td>
      <td>38.000000</td>
      <td>3.200000</td>
      <td>4.380000</td>
      <td>34.000000</td>
      <td>63.000000</td>
      <td>151.000000</td>
      <td>121.250000</td>
      <td>75.000000</td>
      <td>138.000000</td>
      <td>84.000000</td>
      <td>33.000000</td>
      <td>39.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>204.000000</td>
      <td>46.000000</td>
      <td>4.200000</td>
      <td>4.840000</td>
      <td>45.000000</td>
      <td>66.000000</td>
      <td>172.500000</td>
      <td>136.000000</td>
      <td>82.000000</td>
      <td>149.000000</td>
      <td>92.000000</td>
      <td>37.000000</td>
      <td>42.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>230.000000</td>
      <td>59.000000</td>
      <td>5.400000</td>
      <td>5.600000</td>
      <td>60.000000</td>
      <td>69.000000</td>
      <td>200.000000</td>
      <td>146.750000</td>
      <td>90.000000</td>
      <td>161.000000</td>
      <td>100.000000</td>
      <td>41.000000</td>
      <td>46.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>443.000000</td>
      <td>120.000000</td>
      <td>19.299999</td>
      <td>16.110001</td>
      <td>92.000000</td>
      <td>76.000000</td>
      <td>325.000000</td>
      <td>250.000000</td>
      <td>124.000000</td>
      <td>238.000000</td>
      <td>124.000000</td>
      <td>56.000000</td>
      <td>64.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>From the summary statistics and the visualizations I can conclude that all features look OK. I do not see any extreme values for any feature.</p>
<h3>Distributions of Categorical Features</h3>
<div class="highlight"><pre><span></span><span class="c1"># Summarize categorical features</span>
<span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">'object'</span><span class="p">])</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>frame</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>403</td>
      <td>391</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>top</th>
      <td>female</td>
      <td>medium</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>234</td>
      <td>184</td>
    </tr>
  </tbody>
</table>
</div>

<p>This shows us the number of unique classes for each feature. For example, there are more females (234) than males. And <code>'medium'</code> is most common body frame. There are no sparse classes.</p>
<p>Let's visualize this information, using Seaborn <code>.countplot()</code> function.</p>
<div class="highlight"><pre><span></span><span class="c1"># Bar plot for 'gender'</span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'gender'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Display count of each class</span>
<span class="n">Counter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">gender</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="http://localhost:8000/images//images/output_39_0.png"></p>
<div class="highlight"><pre><span></span>Counter({'female': 234, 'male': 169})
</pre></div>


<p>No missing data for the <code style="color:steelblue">gender</code> feature and my sample has more females.</p>
<div class="highlight"><pre><span></span><span class="c1"># Bar plot for 'frame'</span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'frame'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="p">[</span><span class="s1">'small'</span><span class="p">,</span> <span class="s1">'medium'</span><span class="p">,</span> <span class="s1">'large'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Display count of each class</span>
<span class="n">Counter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">frame</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_41_0.png"></p>
<div class="highlight"><pre><span></span>Counter({'medium': 184, 'large': 103, 'small': 104, nan: 12})
</pre></div>


<p>Medium body frame is most common and almost the same number of participants belongs to the small and large body frame classes. I can also see that there are 12 missing values.</p>
<h3>Segmentations</h3>
<p>Later on I will encode <code style="color:steelblue">a1c</code> to 3 classes and this will become my target variable. But, for the moment I can treat <code style="color:steelblue">a1c</code> as the target.</p>
<p>First I will segment <code style="color:steelblue">a1c</code> by <code>gender</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># Segment a1c by gender and plot distributions</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s1">'gender'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">'a1c'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_45_0.png"></p>
<div class="highlight"><pre><span></span><span class="c1"># Segment a1c by gender and display the means within each class</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'gender'</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>height</th>
      <th>weight</th>
      <th>bp_1s</th>
      <th>bp_1d</th>
      <th>bp_2s</th>
      <th>bp_2d</th>
      <th>waist</th>
      <th>hip</th>
    </tr>
    <tr>
      <th>gender</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>female</th>
      <td>208.435897</td>
      <td>52.111111</td>
      <td>4.355983</td>
      <td>5.494342</td>
      <td>45.833333</td>
      <td>63.733624</td>
      <td>174.487179</td>
      <td>136.307359</td>
      <td>82.484848</td>
      <td>153.350649</td>
      <td>91.753247</td>
      <td>38.124464</td>
      <td>44.347639</td>
    </tr>
    <tr>
      <th>male</th>
      <td>207.023810</td>
      <td>48.125000</td>
      <td>4.752381</td>
      <td>5.724074</td>
      <td>48.260355</td>
      <td>69.118343</td>
      <td>181.916667</td>
      <td>137.730539</td>
      <td>84.479042</td>
      <td>151.218750</td>
      <td>93.453125</td>
      <td>37.589286</td>
      <td>41.226190</td>
    </tr>
  </tbody>
</table>
</div>

<p>It is not easy to see from the plot, but grouping by gender can show us that average <code style="color:steelblue">a1c</code> for females is slightly lower tthan for males.</p>
<p>Next, let's segment <code style="color:steelblue">a1c</code> by body frame.</p>
<div class="highlight"><pre><span></span><span class="c1"># Segment a1c by frame and plot distributions</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s1">'frame'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">'a1c'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_49_0.png"></p>
<div class="highlight"><pre><span></span><span class="c1"># Segment by frame and display the means within each class</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'frame'</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>height</th>
      <th>weight</th>
      <th>bp_1s</th>
      <th>bp_1d</th>
      <th>bp_2s</th>
      <th>bp_2d</th>
      <th>waist</th>
      <th>hip</th>
    </tr>
    <tr>
      <th>frame</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>large</th>
      <td>208.000000</td>
      <td>44.601942</td>
      <td>4.992233</td>
      <td>6.105657</td>
      <td>52.844660</td>
      <td>66.643564</td>
      <td>203.805825</td>
      <td>141.184466</td>
      <td>83.398058</td>
      <td>154.513514</td>
      <td>93.648649</td>
      <td>41.823529</td>
      <td>45.735294</td>
    </tr>
    <tr>
      <th>medium</th>
      <td>213.538043</td>
      <td>50.798913</td>
      <td>4.638043</td>
      <td>5.640281</td>
      <td>45.706522</td>
      <td>65.701087</td>
      <td>178.311475</td>
      <td>137.762431</td>
      <td>84.950276</td>
      <td>151.506667</td>
      <td>91.680000</td>
      <td>37.890710</td>
      <td>43.360656</td>
    </tr>
    <tr>
      <th>small</th>
      <td>197.495146</td>
      <td>55.543689</td>
      <td>3.845631</td>
      <td>5.040882</td>
      <td>42.211538</td>
      <td>66.088235</td>
      <td>150.961538</td>
      <td>130.029126</td>
      <td>79.980583</td>
      <td>153.318182</td>
      <td>93.681818</td>
      <td>34.048077</td>
      <td>39.778846</td>
    </tr>
  </tbody>
</table>
</div>

<p>This time I can see from the plot and from grouping that average <code style="color:steelblue">a1c</code> increases with body frame size.</p>
<p>We will keep all outliers. They just represent high and, for some observations, extreme values of <code style="color:steelblue">a1c</code>.</p>
<h3>Correlations</h3>
<p>Let's calculate correlations to take a look at the relationships between numeric features and other numeric features.</p>
<p>We are going to sort features in order of their correlation with <code style="color:steelblue">a1c</code>. This will make easier to see stronger correlations for <code style="color:steelblue">a1c</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># Calculate correlations between numeric features</span>
<span class="n">correlations</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c1"># sort features in order of their correlation with a1c</span>
<span class="n">sort_corr_cols</span> <span class="o">=</span> <span class="n">correlations</span><span class="o">.</span><span class="n">a1c</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="n">sort_corr</span> <span class="o">=</span> <span class="n">correlations</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">sort_corr_cols</span><span class="p">,</span><span class="n">sort_corr_cols</span><span class="p">]</span>
<span class="n">sort_corr</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a1c</th>
      <th>age</th>
      <th>chol_ratio</th>
      <th>chol</th>
      <th>waist</th>
      <th>bp_1s</th>
      <th>weight</th>
      <th>hip</th>
      <th>height</th>
      <th>bp_2s</th>
      <th>bp_1d</th>
      <th>hdl</th>
      <th>bp_2d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a1c</th>
      <td>1.000000</td>
      <td>0.338929</td>
      <td>0.328665</td>
      <td>0.247099</td>
      <td>0.226184</td>
      <td>0.197936</td>
      <td>0.171882</td>
      <td>0.141401</td>
      <td>0.063023</td>
      <td>0.042671</td>
      <td>0.032375</td>
      <td>-0.149145</td>
      <td>-0.160241</td>
    </tr>
    <tr>
      <th>age</th>
      <td>0.338929</td>
      <td>1.000000</td>
      <td>0.148510</td>
      <td>0.233119</td>
      <td>0.149645</td>
      <td>0.443041</td>
      <td>-0.055970</td>
      <td>0.008819</td>
      <td>-0.090493</td>
      <td>0.366340</td>
      <td>0.058227</td>
      <td>0.038088</td>
      <td>-0.134088</td>
    </tr>
    <tr>
      <th>chol_ratio</th>
      <td>0.328665</td>
      <td>0.148510</td>
      <td>1.000000</td>
      <td>0.475521</td>
      <td>0.304162</td>
      <td>0.119386</td>
      <td>0.281649</td>
      <td>0.194622</td>
      <td>0.094335</td>
      <td>-0.048257</td>
      <td>0.048193</td>
      <td>-0.686907</td>
      <td>-0.155987</td>
    </tr>
    <tr>
      <th>chol</th>
      <td>0.247099</td>
      <td>0.233119</td>
      <td>0.475521</td>
      <td>1.000000</td>
      <td>0.124489</td>
      <td>0.203344</td>
      <td>0.066889</td>
      <td>0.079402</td>
      <td>-0.058858</td>
      <td>0.080418</td>
      <td>0.171605</td>
      <td>0.186581</td>
      <td>0.003482</td>
    </tr>
    <tr>
      <th>waist</th>
      <td>0.226184</td>
      <td>0.149645</td>
      <td>0.304162</td>
      <td>0.124489</td>
      <td>1.000000</td>
      <td>0.196489</td>
      <td>0.849855</td>
      <td>0.837080</td>
      <td>0.051094</td>
      <td>0.093171</td>
      <td>0.167110</td>
      <td>-0.268369</td>
      <td>0.048802</td>
    </tr>
    <tr>
      <th>bp_1s</th>
      <td>0.197936</td>
      <td>0.443041</td>
      <td>0.119386</td>
      <td>0.203344</td>
      <td>0.196489</td>
      <td>1.000000</td>
      <td>0.090873</td>
      <td>0.136655</td>
      <td>-0.047827</td>
      <td>0.868773</td>
      <td>0.596557</td>
      <td>0.019804</td>
      <td>0.291365</td>
    </tr>
    <tr>
      <th>weight</th>
      <td>0.171882</td>
      <td>-0.055970</td>
      <td>0.281649</td>
      <td>0.066889</td>
      <td>0.849855</td>
      <td>0.090873</td>
      <td>1.000000</td>
      <td>0.829115</td>
      <td>0.251251</td>
      <td>-0.071383</td>
      <td>0.175956</td>
      <td>-0.290983</td>
      <td>0.041657</td>
    </tr>
    <tr>
      <th>hip</th>
      <td>0.141401</td>
      <td>0.008819</td>
      <td>0.194622</td>
      <td>0.079402</td>
      <td>0.837080</td>
      <td>0.136655</td>
      <td>0.829115</td>
      <td>1.000000</td>
      <td>-0.107832</td>
      <td>0.013381</td>
      <td>0.145805</td>
      <td>-0.210060</td>
      <td>0.045458</td>
    </tr>
    <tr>
      <th>height</th>
      <td>0.063023</td>
      <td>-0.090493</td>
      <td>0.094335</td>
      <td>-0.058858</td>
      <td>0.051094</td>
      <td>-0.047827</td>
      <td>0.251251</td>
      <td>-0.107832</td>
      <td>1.000000</td>
      <td>-0.060529</td>
      <td>0.038598</td>
      <td>-0.101419</td>
      <td>0.077816</td>
    </tr>
    <tr>
      <th>bp_2s</th>
      <td>0.042671</td>
      <td>0.366340</td>
      <td>-0.048257</td>
      <td>0.080418</td>
      <td>0.093171</td>
      <td>0.868773</td>
      <td>-0.071383</td>
      <td>0.013381</td>
      <td>-0.060529</td>
      <td>1.000000</td>
      <td>0.305244</td>
      <td>0.128762</td>
      <td>0.420709</td>
    </tr>
    <tr>
      <th>bp_1d</th>
      <td>0.032375</td>
      <td>0.058227</td>
      <td>0.048193</td>
      <td>0.171605</td>
      <td>0.167110</td>
      <td>0.596557</td>
      <td>0.175956</td>
      <td>0.145805</td>
      <td>0.038598</td>
      <td>0.305244</td>
      <td>1.000000</td>
      <td>0.065732</td>
      <td>0.757062</td>
    </tr>
    <tr>
      <th>hdl</th>
      <td>-0.149145</td>
      <td>0.038088</td>
      <td>-0.686907</td>
      <td>0.186581</td>
      <td>-0.268369</td>
      <td>0.019804</td>
      <td>-0.290983</td>
      <td>-0.210060</td>
      <td>-0.101419</td>
      <td>0.128762</td>
      <td>0.065732</td>
      <td>1.000000</td>
      <td>0.147352</td>
    </tr>
    <tr>
      <th>bp_2d</th>
      <td>-0.160241</td>
      <td>-0.134088</td>
      <td>-0.155987</td>
      <td>0.003482</td>
      <td>0.048802</td>
      <td>0.291365</td>
      <td>0.041657</td>
      <td>0.045458</td>
      <td>0.077816</td>
      <td>0.420709</td>
      <td>0.757062</td>
      <td>0.147352</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>It is not easy to digest this big dataframe.</p>
<p>Let's use Seaborn's <code>.heatmap()</code> function to visualize the correlation grid.</p>
<div class="highlight"><pre><span></span><span class="c1"># Generate a mask for the upper triangle</span>
<span class="n">corr_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">correlations</span><span class="p">)</span>
<span class="n">corr_mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">corr_mask</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Make the figsize 9x9</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>

<span class="c1"># Plot heatmap of annotated correlations; change background to white</span>
<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">axes_style</span><span class="p">(</span><span class="s1">'white'</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">sort_corr</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
                <span class="n">cmap</span><span class="o">=</span><span class="s1">'RdBu_r'</span><span class="p">,</span> 
                <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">fmt</span><span class="o">=</span><span class="s1">'.0f'</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">corr_mask</span><span class="p">,</span>
                <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Correlations by A1C'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_56_0.png"></p>
<p>We can see few obvious stronger correlations:</p>
<ul>
<li>
<p><code style="color:steelblue">weight</code> and <code style="color:steelblue">waist</code> and <code style="color:steelblue">hip</code></p>
</li>
<li>
<p>blood presure measurements</p>
</li>
<li>
<p>cholesterol ratio and <code style="color:steelblue">chol</code> and <code style="color:steelblue">hdl</code></p>
</li>
</ul>
<p>But there are only few week correlations with <code style="color:steelblue">a1c</code>:</p>
<ul>
<li><code style="color:steelblue">chol_ratio</code>, <code style="color:steelblue">age</code>, <code style="color:steelblue">waist</code> ...</li>
</ul>
<h2>Data Cleaning</h2>
<h3>Duplicate observations</h3>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Before:'</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Drop duplicates</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Before: (403, 15)
(403, 15)
</pre></div>


<p>There were no duplicates in the dataframe.</p>
<h3>Missing Data</h3>
<div class="highlight"><pre><span></span><span class="c1"># Display number of missing values by feature</span>
<span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>chol            1
hdl             1
chol_ratio      1
a1c            13
age             0
gender          0
height          5
weight          1
frame          12
bp_1s           5
bp_1d           5
bp_2s         262
bp_2d         262
waist           2
hip             2
dtype: int64
</pre></div>


<ul>
<li>
<p>Only <code style="color:steelblue">gender</code> and <code style="color:steelblue">age</code> 
features do not have any missing values. </p>
</li>
<li>
<p>The second blood presure observations have plenty of missing values.</p>
</li>
</ul>
<p>First, let's take care of the target variable.</p>
<p><strong>Handling missing values for the target value</strong></p>
<p>Our target variable will be constructed later from the values in the <code style="color:steelblue">a1c</code> feature.</p>
<p>We can see that there are 13 missing values for <code style="color:steelblue">a1c</code>, so let's remove those observations.</p>
<div class="highlight"><pre><span></span><span class="c1"># df.drop(df[df['a1c'].isnull()].index, inplace=True)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">'a1c'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(390, 15)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>chol            1
hdl             1
chol_ratio      1
a1c             0
age             0
gender          0
height          5
weight          1
frame          11
bp_1s           5
bp_1d           5
bp_2s         252
bp_2d         252
waist           2
hip             2
dtype: int64
</pre></div>


<p>The number of observations has ben decrased to 390.</p>
<p>Next, I are going to check categorical features for missinig data.</p>
<p><strong>Find and count the missing <em>categorical</em> data</strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Count the number of missing values for categorical features</span>
<span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">'object'</span><span class="p">])</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>gender     0
frame     11
dtype: int64
</pre></div>


<p>The <code style="color:steelblue">frame</code> feature has 11 missing values. Let's take care of this.</p>
<p><strong>Label missing categorical values as 'Missing'</strong></p>
<p>We will handle this by simply labeling missing values as 'Missing'.</p>
<div class="highlight"><pre><span></span><span class="c1"># fill missing values in frame with 'Missing</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'frame'</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">'Missing'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>We can confirm now that the <code style="color:steelblue">frame</code> feature does not have any missing values:</p>
<div class="highlight"><pre><span></span><span class="c1"># Count the number of missing values for categorical features</span>
<span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">'object'</span><span class="p">])</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>gender    0
frame     0
dtype: int64
</pre></div>


<p>The new class <code>'Missing'</code> has been added to the <code style="color:steelblue">frame</code> feature:</p>
<div class="highlight"><pre><span></span><span class="c1"># Display the count for each class</span>
<span class="n">df</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>medium     178
small      102
large       99
Missing     11
Name: frame, dtype: int64
</pre></div>


<p><strong>Find and count the missing <em>numerical</em> data</strong></p>
<p>Next, I will continue with missing values for numerical features.</p>
<div class="highlight"><pre><span></span><span class="c1"># Count the number of missing values for numerical features</span>
<span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s1">'object'</span><span class="p">])</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>chol            1
hdl             1
chol_ratio      1
a1c             0
age             0
height          5
weight          1
bp_1s           5
bp_1d           5
bp_2s         252
bp_2d         252
waist           2
hip             2
dtype: int64
</pre></div>


<p>First I will take care of four blood preasure features.</p>
<p><strong><em><u>Blood Presure Measurements</u></em></strong></p>
<p>Let's check observations for missing data of the first blood preasure measurments.</p>
<p>There should be 5 observations and I want to check the status of the second blood preasure measuremnets in them.</p>
<div class="highlight"><pre><span></span><span class="c1"># Display observations with bp_1s missing</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">bp_1s</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>bp_1s</th>
      <th>bp_1d</th>
      <th>bp_2s</th>
      <th>bp_2d</th>
      <th>waist</th>
      <th>hip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7</th>
      <td>227.0</td>
      <td>44.0</td>
      <td>5.2</td>
      <td>3.94</td>
      <td>37</td>
      <td>male</td>
      <td>59.0</td>
      <td>170.0</td>
      <td>medium</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>34.0</td>
      <td>39.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>183.0</td>
      <td>46.0</td>
      <td>4.0</td>
      <td>4.59</td>
      <td>40</td>
      <td>female</td>
      <td>59.0</td>
      <td>165.0</td>
      <td>medium</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>37.0</td>
      <td>43.0</td>
    </tr>
    <tr>
      <th>37</th>
      <td>190.0</td>
      <td>32.0</td>
      <td>5.9</td>
      <td>3.56</td>
      <td>46</td>
      <td>male</td>
      <td>72.0</td>
      <td>205.0</td>
      <td>medium</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>46.0</td>
      <td>49.0</td>
    </tr>
    <tr>
      <th>63</th>
      <td>145.0</td>
      <td>29.0</td>
      <td>5.0</td>
      <td>3.99</td>
      <td>38</td>
      <td>female</td>
      <td>NaN</td>
      <td>125.0</td>
      <td>Missing</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>31.0</td>
      <td>35.0</td>
    </tr>
    <tr>
      <th>215</th>
      <td>197.0</td>
      <td>46.0</td>
      <td>4.3</td>
      <td>4.75</td>
      <td>36</td>
      <td>female</td>
      <td>64.0</td>
      <td>136.0</td>
      <td>small</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>32.0</td>
      <td>37.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>There are 252 missing data for the second blood presure measurement and 5 for the first measurement. I will create a new features,<code style="color:steelblue"> bp_s</code> and <code style="color:steelblue">bp_d</code>, and at the end this will handle missing data. </p>
<ul>
<li>
<p><em>bp_1s</em> &amp; <em>bp_2s</em> → <strong>bp_s</strong></p>
</li>
<li>
<p><em>bp_1d</em> &amp; <em>bp_2d</em> → <strong>bp_d</strong></p>
</li>
</ul>
<p>The new features are average of two measurements for both, systolic and diastolic, blood presures respectively. The mean will take care of most of the <code>NaN</code>s, except for the observations listed above. </p>
<p>Because all blood preasure measurements are missing, the newly constracted features will still have missing values for those 5 observations.</p>
<div class="highlight"><pre><span></span><span class="c1"># Create 2 new features, bp_s and bp_d</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'bp_s'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">'bp_1s'</span><span class="p">,</span> <span class="s1">'bp_2s'</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'bp_d'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">'bp_1d'</span><span class="p">,</span> <span class="s1">'bp_2d'</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>bp_1s</th>
      <th>bp_1d</th>
      <th>bp_2s</th>
      <th>bp_2d</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>62.0</td>
      <td>121.0</td>
      <td>medium</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>38.0</td>
      <td>118.0</td>
      <td>59.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>64.0</td>
      <td>218.0</td>
      <td>large</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>46.0</td>
      <td>48.0</td>
      <td>112.0</td>
      <td>68.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now I can drop the old features:</p>
<div class="highlight"><pre><span></span><span class="c1"># Drop the old features: bp_1s, bp_2s, bp_1d, bp_2d </span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'bp_1s'</span><span class="p">,</span> <span class="s1">'bp_2s'</span><span class="p">,</span> <span class="s1">'bp_1d'</span><span class="p">,</span> <span class="s1">'bp_2d'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>62.0</td>
      <td>121.0</td>
      <td>medium</td>
      <td>29.0</td>
      <td>38.0</td>
      <td>118.0</td>
      <td>59.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>64.0</td>
      <td>218.0</td>
      <td>large</td>
      <td>46.0</td>
      <td>48.0</td>
      <td>112.0</td>
      <td>68.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="c1"># Display observations with bp_s missing</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">bp_s</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7</th>
      <td>227.0</td>
      <td>44.0</td>
      <td>5.2</td>
      <td>3.94</td>
      <td>37</td>
      <td>male</td>
      <td>59.0</td>
      <td>170.0</td>
      <td>medium</td>
      <td>34.0</td>
      <td>39.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>13</th>
      <td>183.0</td>
      <td>46.0</td>
      <td>4.0</td>
      <td>4.59</td>
      <td>40</td>
      <td>female</td>
      <td>59.0</td>
      <td>165.0</td>
      <td>medium</td>
      <td>37.0</td>
      <td>43.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>37</th>
      <td>190.0</td>
      <td>32.0</td>
      <td>5.9</td>
      <td>3.56</td>
      <td>46</td>
      <td>male</td>
      <td>72.0</td>
      <td>205.0</td>
      <td>medium</td>
      <td>46.0</td>
      <td>49.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>63</th>
      <td>145.0</td>
      <td>29.0</td>
      <td>5.0</td>
      <td>3.99</td>
      <td>38</td>
      <td>female</td>
      <td>NaN</td>
      <td>125.0</td>
      <td>Missing</td>
      <td>31.0</td>
      <td>35.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>215</th>
      <td>197.0</td>
      <td>46.0</td>
      <td>4.3</td>
      <td>4.75</td>
      <td>36</td>
      <td>female</td>
      <td>64.0</td>
      <td>136.0</td>
      <td>small</td>
      <td>32.0</td>
      <td>37.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<p>We just confirmed what I predicted earlier: 5 observations have still missing blood preasure values.</p>
<p>We will take care of them with the rest of missing data which I are going to display and count right now:</p>
<p><strong><em><u>Cholesterol Measurements</u></em></strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Display and count observations with cholesetrol values missing</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[[</span><span class="s1">'chol'</span><span class="p">,</span> <span class="s1">'hdl'</span><span class="p">,</span> <span class="s1">'chol_ratio'</span><span class="p">]]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">'chol'</span><span class="p">,</span> <span class="s1">'hdl'</span><span class="p">,</span> <span class="s1">'chol_ratio'</span><span class="p">]]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>27</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>4.28</td>
      <td>48</td>
      <td>male</td>
      <td>68.0</td>
      <td>100.0</td>
      <td>small</td>
      <td>27.0</td>
      <td>33.0</td>
      <td>120.0</td>
      <td>85.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>chol          1
hdl           1
chol_ratio    1
dtype: int64
</pre></div>


<p><strong><em><u>Waist and Hip Measurements</u></em></strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Display and count observations with waist and hip values missing</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[[</span><span class="s1">'waist'</span><span class="p">,</span> <span class="s1">'hip'</span><span class="p">]]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">'waist'</span><span class="p">,</span> <span class="s1">'hip'</span><span class="p">]]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>336</th>
      <td>158.0</td>
      <td>64.0</td>
      <td>2.5</td>
      <td>2.73</td>
      <td>30</td>
      <td>female</td>
      <td>62.0</td>
      <td>142.0</td>
      <td>medium</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>108.0</td>
      <td>68.0</td>
    </tr>
    <tr>
      <th>393</th>
      <td>192.0</td>
      <td>69.0</td>
      <td>2.8</td>
      <td>4.38</td>
      <td>51</td>
      <td>male</td>
      <td>65.0</td>
      <td>146.0</td>
      <td>large</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>150.0</td>
      <td>114.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>waist    2
hip      2
dtype: int64
</pre></div>


<p><strong><em><u>Height and Weight Measurements</u></em></strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Display and count observations with height or weight missing</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">height</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span> <span class="o">|</span> <span class="n">df</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">isnull</span><span class="p">()])</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">'weight'</span><span class="p">,</span> <span class="s1">'height'</span><span class="p">]]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>63</th>
      <td>145.0</td>
      <td>29.0</td>
      <td>5.0</td>
      <td>3.99</td>
      <td>38</td>
      <td>female</td>
      <td>NaN</td>
      <td>125.0</td>
      <td>Missing</td>
      <td>31.0</td>
      <td>35.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>86</th>
      <td>218.0</td>
      <td>54.0</td>
      <td>4.0</td>
      <td>10.55</td>
      <td>51</td>
      <td>female</td>
      <td>NaN</td>
      <td>215.0</td>
      <td>large</td>
      <td>42.0</td>
      <td>53.0</td>
      <td>139.0</td>
      <td>69.0</td>
    </tr>
    <tr>
      <th>161</th>
      <td>115.0</td>
      <td>36.0</td>
      <td>3.2</td>
      <td>13.60</td>
      <td>58</td>
      <td>male</td>
      <td>69.0</td>
      <td>NaN</td>
      <td>medium</td>
      <td>30.0</td>
      <td>37.0</td>
      <td>125.0</td>
      <td>69.0</td>
    </tr>
    <tr>
      <th>195</th>
      <td>173.0</td>
      <td>37.0</td>
      <td>4.7</td>
      <td>4.31</td>
      <td>40</td>
      <td>female</td>
      <td>NaN</td>
      <td>130.0</td>
      <td>small</td>
      <td>37.0</td>
      <td>38.0</td>
      <td>122.0</td>
      <td>76.0</td>
    </tr>
    <tr>
      <th>231</th>
      <td>214.0</td>
      <td>35.0</td>
      <td>6.1</td>
      <td>5.38</td>
      <td>44</td>
      <td>female</td>
      <td>NaN</td>
      <td>190.0</td>
      <td>large</td>
      <td>38.0</td>
      <td>44.0</td>
      <td>140.0</td>
      <td>75.0</td>
    </tr>
    <tr>
      <th>317</th>
      <td>300.0</td>
      <td>59.0</td>
      <td>5.1</td>
      <td>4.56</td>
      <td>34</td>
      <td>female</td>
      <td>NaN</td>
      <td>160.0</td>
      <td>small</td>
      <td>40.0</td>
      <td>47.0</td>
      <td>120.0</td>
      <td>60.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>weight    1
height    5
dtype: int64
</pre></div>


<p><strong><em><u>Calculate number of observations with missing values</u></em></strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Count the number of missing values for numerical features</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s1">'object'</span><span class="p">])</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Count the total number of missing values </span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Total number of missing values (NaNs):'</span><span class="p">,</span>
      <span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s1">'object'</span><span class="p">])</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Number of observations with at least one NaN</span>
<span class="n">row_nan</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Observations with at least 1 Nan:'</span><span class="p">,</span> <span class="n">row_nan</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>chol          1
hdl           1
chol_ratio    1
a1c           0
age           0
height        5
weight        1
waist         2
hip           2
bp_s          5
bp_d          5
dtype: int64

Total number of missing values (NaNs): 23

Observations with at least 1 Nan: 13
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Current shape of dataframe</span>
<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(390, 13)
</pre></div>


<p>The 23 missing values are spread across 9 of 11 numerical features and 13 observations. The proportion of impacted observations is small relative to the entire dataset.</p>
<p>This is why I are going to drop those 13 observations.</p>
<div class="highlight"><pre><span></span><span class="c1"># Drop all observations with missing values</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(377, 13)
</pre></div>


<p>Confirm that there are no missing values.</p>
<div class="highlight"><pre><span></span><span class="c1"># Count missing values</span>
<span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>chol          0
hdl           0
chol_ratio    0
a1c           0
age           0
gender        0
height        0
weight        0
frame         0
waist         0
hip           0
bp_s          0
bp_d          0
dtype: int64
</pre></div>


<p>Yes, my dataset does not have any missing values. </p>
<p>The number of observations is now 377. </p>
<h4>Index reset</h4>
<p>After I finished with removing observations, I should reset the index.</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">index</span>
</pre></div>


<div class="highlight"><pre><span></span>Int64Index([  0,   1,   2,   3,   4,   5,   6,   8,   9,  10,
            ...
            390, 391, 392, 394, 395, 397, 398, 399, 400, 401],
           dtype='int64', length=377)
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># index reset</span>
<span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">index</span>
</pre></div>


<div class="highlight"><pre><span></span>RangeIndex(start=0, stop=377, step=1)
</pre></div>


<h2>Feature Engineering</h2>
<p>Let's display the first 5 observations of the dataset .</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>height</th>
      <th>weight</th>
      <th>frame</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>62.0</td>
      <td>121.0</td>
      <td>medium</td>
      <td>29.0</td>
      <td>38.0</td>
      <td>118.0</td>
      <td>59.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>64.0</td>
      <td>218.0</td>
      <td>large</td>
      <td>46.0</td>
      <td>48.0</td>
      <td>112.0</td>
      <td>68.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>4.64</td>
      <td>58</td>
      <td>female</td>
      <td>61.0</td>
      <td>256.0</td>
      <td>large</td>
      <td>49.0</td>
      <td>57.0</td>
      <td>187.5</td>
      <td>92.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>78.0</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>4.63</td>
      <td>67</td>
      <td>male</td>
      <td>67.0</td>
      <td>119.0</td>
      <td>large</td>
      <td>33.0</td>
      <td>38.0</td>
      <td>110.0</td>
      <td>50.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>249.0</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>7.72</td>
      <td>64</td>
      <td>male</td>
      <td>68.0</td>
      <td>183.0</td>
      <td>medium</td>
      <td>44.0</td>
      <td>41.0</td>
      <td>138.0</td>
      <td>80.0</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Creating New Features</h3>
<p>We will combine the following feature pairs to create two new features:</p>
<ul>
<li>
<p><em>height</em> &amp; <em>weight</em> → <strong>bmi</strong></p>
</li>
<li>
<p><em>waist</em> &amp; <em>hip</em> → <strong>whr</strong></p>
</li>
</ul>
<h4>Body Mass Index (BMI)</h4>
<p>In general, BMI is an inexpensive and easy-to-perform method of screening for weight category, for example underweight, normal or healthy weight, overweight, and obesity. </p>
<table>
<thead>
<tr>
<th>BMI</th>
<th>Weight Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Below 18.5</td>
<td>Underweight</td>
</tr>
<tr>
<td>18.5 – 24.9</td>
<td>Normal or Healthy Weight</td>
</tr>
<tr>
<td>25.0 – 29.9</td>
<td>Overweight</td>
</tr>
<tr>
<td>30.0 and Above</td>
<td>Obese</td>
</tr>
</tbody>
</table>
<p>BMI appears to be as strongly correlated with various metabolic and disease outcome as are
these more direct measures of body fatness. It is inexpensive and easy to use because its calculation requires only height and weight.</p>
<p>We will use the <code style="color:steelblue">height</code> and <code style="color:steelblue">weight</code> features to create one new feature, <code style="color:steelblue">bmi</code>.</p>
<p>To calculate BMI I use the following formula:</p>
<p>$
\begin{align}
\frac{weight}{height^2}\times703
\end{align}
$</p>
<p>After that I will drop the old features, <code style="color:steelblue">height</code> and <code style="color:steelblue">weight</code>.</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">'bmi'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(((</span><span class="n">df</span><span class="o">.</span><span class="n">weight</span> <span class="o">/</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">height</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">703</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'height'</span><span class="p">,</span> <span class="s1">'weight'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>frame</th>
      <th>waist</th>
      <th>hip</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>medium</td>
      <td>29.0</td>
      <td>38.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>large</td>
      <td>46.0</td>
      <td>48.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
    </tr>
  </tbody>
</table>
</div>

<h4>Waist-Hip Ratio</h4>
<p>Physical inactivity and obesity have been well recognized as major lifestyle related risk factors for diabetes. Several obesity indicators like waist circumference (WC), body mass index (BMI) and waist-to-hip ratio (WHR) are considered to be related to the incidence and prevalence of type II diabetes in adults.</p>
<p>This chart shows how the WHO classifies the risk of being affected by weight related health conditions according to waist-to-hip ratio:</p>
<table>
<thead>
<tr>
<th align="center">Health risk</th>
<th>Men</th>
<th align="right">Women</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Low</td>
<td>0.95 or lower</td>
<td align="right">0.80 or lower</td>
</tr>
<tr>
<td align="center">Moderate</td>
<td>0.96-1.0</td>
<td align="right">0.81-0.85</td>
</tr>
<tr>
<td align="center">High</td>
<td>1.0 or higher</td>
<td align="right">0.86 or higher</td>
</tr>
</tbody>
</table>
<p>Next, I will create the new feature <code style="color:steelblue">whr</code> for waist-to-hip ratio:</p>
<p>$
\begin{align}
whr = \frac{waist}{hip}
\end{align}
$</p>
<p>We will keep the <code style="color:steelblue">waist</code> feature and drop the <code style="color:steelblue">hip</code> feature.</p>
<div class="highlight"><pre><span></span><span class="c1"># create a new whr feature</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'whr'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">((</span><span class="n">df</span><span class="o">.</span><span class="n">waist</span> <span class="o">/</span> <span class="n">df</span><span class="o">.</span><span class="n">hip</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># drop hip feature</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'hip'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>frame</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>medium</td>
      <td>29.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
      <td>0.76</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>large</td>
      <td>46.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
      <td>0.96</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="c1"># Current shape of dataframe</span>
<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(377, 12)
</pre></div>


<p>The number of features is now 12.</p>
<h4>Type 2 Diabetes - T2D</h4>
<p>The A1C test is used to diagnose diabetes. For people with diabetes it can tell if they need to adjust their diabetes medicine.</p>
<p>The following table lists A1C levels:</p>
<table>
<thead>
<tr>
<th align="center">A1C [%]</th>
<th>Health risk</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">below 5.7</td>
<td>No diabetes</td>
</tr>
<tr>
<td align="center">5.7 to 6.4</td>
<td>Pre-diabetes</td>
</tr>
<tr>
<td align="center">6.5 or higher</td>
<td>Diabetes</td>
</tr>
</tbody>
</table>
<p>We will group <code style="color:steelblue">a1c</code> values into three buckets using the above table and create a new feature <code style="color:steelblue">health_risk</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># create health_risk feature from a1c</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'health_risk'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">a1c</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="s1">'no_diabetes'</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mf">5.7</span> \
                         <span class="k">else</span> <span class="s1">'diabetes'</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mf">6.5</span> <span class="k">else</span> <span class="s1">'pre_diabetes'</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>frame</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
      <th>health_risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>medium</td>
      <td>29.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
      <td>0.76</td>
      <td>no_diabetes</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>large</td>
      <td>46.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
      <td>0.96</td>
      <td>no_diabetes</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>4.64</td>
      <td>58</td>
      <td>female</td>
      <td>large</td>
      <td>49.0</td>
      <td>187.5</td>
      <td>92.0</td>
      <td>48.4</td>
      <td>0.86</td>
      <td>no_diabetes</td>
    </tr>
  </tbody>
</table>
</div>

<p>The <code style="color:steelblue">health_risk</code> is now my <strong>target variable</strong> and this will become a <strong>multi-class classification task</strong>. </p>
<p><strong>Display the class distributions for the <code style="color:steelblue">health_risk</code> feature</strong> </p>
<div class="highlight"><pre><span></span><span class="c1"># Class distribution for health_risk </span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'health_risk'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
              <span class="n">order</span><span class="o">=</span><span class="p">[</span><span class="s1">'no_diabetes'</span><span class="p">,</span> <span class="s1">'pre_diabetes'</span><span class="p">,</span> <span class="s1">'diabetes'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Distribution of helath risk classes'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Health Risk'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_130_0.png"></p>
<p>Let's define a small helper funtcion which displays count and percentage per class of one feature.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">class_count</span><span class="p">(</span><span class="n">ser</span><span class="p">):</span>
    <span class="c1"># input is one column of dataframe = Series</span>
    <span class="c1"># value_counts() produces another Pandas Series</span>
    <span class="n">pd_sr</span> <span class="o">=</span> <span class="n">ser</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ser</span><span class="p">)</span>
    <span class="c1"># get the name of Series</span>
    <span class="n">ser_name</span> <span class="o">=</span> <span class="n">ser</span><span class="o">.</span><span class="n">name</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'{}:'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ser_name</span><span class="p">),</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'Count:'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'%'</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pd_sr</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span><span class="n">pd_sr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s1">'</span><span class="se">\t\t</span><span class="s1">'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">pd_sr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>


<p>Let's now run the above function and display <code style="color:steelblue">health_risk</code> distribution:</p>
<div class="highlight"><pre><span></span><span class="n">class_count</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">health_risk</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">health_risk</span><span class="o">:</span>     <span class="n">Count</span><span class="o">:</span>      <span class="o">%</span>
<span class="n">no_diabetes</span>      <span class="mi">290</span>         <span class="mf">76.92</span>
<span class="n">diabetes</span>     <span class="mi">63</span>          <span class="mf">16.71</span>
<span class="n">pre_diabetes</span>     <span class="mi">24</span>          <span class="mf">6.37</span>
</pre></div>


<p>In my sample the majority of people (76.92%) is in the normal A1C range. Around 6.37% have a higher chance of getting diabetes and 16.71% have diabetes.</p>
<p>We have the IMBALANCED dataset with:</p>
<ul>
<li>
<p><strong>majority class</strong> - <code>no_diabetes</code> </p>
</li>
<li>
<p><strong>minority classes</strong> - <code>pre_diabetes</code> and <code>diabetes</code></p>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>a1c</th>
      <th>age</th>
      <th>gender</th>
      <th>frame</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
      <th>health_risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>4.31</td>
      <td>46</td>
      <td>female</td>
      <td>medium</td>
      <td>29.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
      <td>0.76</td>
      <td>no_diabetes</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>4.44</td>
      <td>29</td>
      <td>female</td>
      <td>large</td>
      <td>46.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
      <td>0.96</td>
      <td>no_diabetes</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>4.64</td>
      <td>58</td>
      <td>female</td>
      <td>large</td>
      <td>49.0</td>
      <td>187.5</td>
      <td>92.0</td>
      <td>48.4</td>
      <td>0.86</td>
      <td>no_diabetes</td>
    </tr>
    <tr>
      <th>3</th>
      <td>78.0</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>4.63</td>
      <td>67</td>
      <td>male</td>
      <td>large</td>
      <td>33.0</td>
      <td>110.0</td>
      <td>50.0</td>
      <td>18.6</td>
      <td>0.87</td>
      <td>no_diabetes</td>
    </tr>
    <tr>
      <th>4</th>
      <td>249.0</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>7.72</td>
      <td>64</td>
      <td>male</td>
      <td>medium</td>
      <td>44.0</td>
      <td>138.0</td>
      <td>80.0</td>
      <td>27.8</td>
      <td>1.07</td>
      <td>diabetes</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Prepare the target variable for models</strong></p>
<p>It is time now to start preparing the target variable <code style="color:steelblue">health_risk</code> for models. I are going to convert non-numeric class labels to numeric labels.</p>
<div class="highlight"><pre><span></span><span class="c1"># convert non-numeric class labels to numeric labels</span>
<span class="c1"># according to the dictionary's mapping</span>
<span class="n">hr</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'no_diabetes'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'pre_diabetes'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'diabetes'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="n">y_hrisk</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'health_risk'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">hr</span><span class="p">)</span>
</pre></div>


<p>Finally, I have a target variable for the models: <code style="color:steelblue">y_hrisk</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># display first 5 labels</span>
<span class="n">y_hrisk</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>0    0
1    0
2    0
3    0
4    2
Name: health_risk, dtype: int64
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># display type of y_hrisk</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">y_hrisk</span><span class="p">))</span>

<span class="c1"># display unique values of y_hrisk</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_hrisk</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;class 'pandas.core.series.Series'&gt;
[0 2 1]
</pre></div>


<p>We have confirmed that <code style="color:steelblue">y_hrisk</code> has 3 unique labels: 0, 1 , 2.</p>
<p>Let's now add it to my dataframe as <code style="color:steelblue">h_risk</code> feature. At the same time I will drop the feature <code style="color:steelblue">a1c</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># drop the a1c column and add previously defined y_hrisk as new numeric feature h_risk</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'a1c'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'h_risk'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_hrisk</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>age</th>
      <th>gender</th>
      <th>frame</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
      <th>health_risk</th>
      <th>h_risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>46</td>
      <td>female</td>
      <td>medium</td>
      <td>29.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
      <td>0.76</td>
      <td>no_diabetes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>29</td>
      <td>female</td>
      <td>large</td>
      <td>46.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
      <td>0.96</td>
      <td>no_diabetes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>58</td>
      <td>female</td>
      <td>large</td>
      <td>49.0</td>
      <td>187.5</td>
      <td>92.0</td>
      <td>48.4</td>
      <td>0.86</td>
      <td>no_diabetes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>78.0</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>67</td>
      <td>male</td>
      <td>large</td>
      <td>33.0</td>
      <td>110.0</td>
      <td>50.0</td>
      <td>18.6</td>
      <td>0.87</td>
      <td>no_diabetes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>249.0</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>64</td>
      <td>male</td>
      <td>medium</td>
      <td>44.0</td>
      <td>138.0</td>
      <td>80.0</td>
      <td>27.8</td>
      <td>1.07</td>
      <td>diabetes</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Checking correlations</h3>
<p>After cleaning dataframe, it is time to check correlations one more time. This time based on the health risk for diabetes.</p>
<p>Again I are going to sort features, but this time in order of their correlation with health risk (<code style="color:steelblue">h_risk</code>). </p>
<div class="highlight"><pre><span></span><span class="c1"># Calculate correlations between numeric features</span>
<span class="n">correlations</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c1"># sort features in order of their correlation with health risk (h_risk)</span>
<span class="n">sort_corr_cols</span> <span class="o">=</span> <span class="n">correlations</span><span class="o">.</span><span class="n">h_risk</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="n">sort_corr</span> <span class="o">=</span> <span class="n">correlations</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">sort_corr_cols</span><span class="p">,</span><span class="n">sort_corr_cols</span><span class="p">]</span>
<span class="n">sort_corr</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>h_risk</th>
      <th>age</th>
      <th>chol_ratio</th>
      <th>waist</th>
      <th>whr</th>
      <th>chol</th>
      <th>bp_s</th>
      <th>bmi</th>
      <th>bp_d</th>
      <th>hdl</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>h_risk</th>
      <td>1.000000</td>
      <td>0.358264</td>
      <td>0.312965</td>
      <td>0.258852</td>
      <td>0.243884</td>
      <td>0.225894</td>
      <td>0.222878</td>
      <td>0.135581</td>
      <td>0.037985</td>
      <td>-0.140111</td>
    </tr>
    <tr>
      <th>age</th>
      <td>0.358264</td>
      <td>1.000000</td>
      <td>0.165961</td>
      <td>0.150054</td>
      <td>0.284849</td>
      <td>0.254901</td>
      <td>0.454645</td>
      <td>-0.011996</td>
      <td>0.074718</td>
      <td>0.027971</td>
    </tr>
    <tr>
      <th>chol_ratio</th>
      <td>0.312965</td>
      <td>0.165961</td>
      <td>1.000000</td>
      <td>0.308856</td>
      <td>0.247888</td>
      <td>0.477736</td>
      <td>0.089232</td>
      <td>0.227548</td>
      <td>0.016238</td>
      <td>-0.683980</td>
    </tr>
    <tr>
      <th>waist</th>
      <td>0.258852</td>
      <td>0.150054</td>
      <td>0.308856</td>
      <td>1.000000</td>
      <td>0.524578</td>
      <td>0.119513</td>
      <td>0.205362</td>
      <td>0.817837</td>
      <td>0.187407</td>
      <td>-0.285098</td>
    </tr>
    <tr>
      <th>whr</th>
      <td>0.243884</td>
      <td>0.284849</td>
      <td>0.247888</td>
      <td>0.524578</td>
      <td>1.000000</td>
      <td>0.100126</td>
      <td>0.146315</td>
      <td>0.108751</td>
      <td>0.078075</td>
      <td>-0.162186</td>
    </tr>
    <tr>
      <th>chol</th>
      <td>0.225894</td>
      <td>0.254901</td>
      <td>0.477736</td>
      <td>0.119513</td>
      <td>0.100126</td>
      <td>1.000000</td>
      <td>0.202727</td>
      <td>0.087380</td>
      <td>0.174391</td>
      <td>0.187278</td>
    </tr>
    <tr>
      <th>bp_s</th>
      <td>0.222878</td>
      <td>0.454645</td>
      <td>0.089232</td>
      <td>0.205362</td>
      <td>0.146315</td>
      <td>0.202727</td>
      <td>1.000000</td>
      <td>0.112855</td>
      <td>0.617349</td>
      <td>0.054596</td>
    </tr>
    <tr>
      <th>bmi</th>
      <td>0.135581</td>
      <td>-0.011996</td>
      <td>0.227548</td>
      <td>0.817837</td>
      <td>0.108751</td>
      <td>0.087380</td>
      <td>0.112855</td>
      <td>1.000000</td>
      <td>0.161525</td>
      <td>-0.246167</td>
    </tr>
    <tr>
      <th>bp_d</th>
      <td>0.037985</td>
      <td>0.074718</td>
      <td>0.016238</td>
      <td>0.187407</td>
      <td>0.078075</td>
      <td>0.174391</td>
      <td>0.617349</td>
      <td>0.161525</td>
      <td>1.000000</td>
      <td>0.095576</td>
    </tr>
    <tr>
      <th>hdl</th>
      <td>-0.140111</td>
      <td>0.027971</td>
      <td>-0.683980</td>
      <td>-0.285098</td>
      <td>-0.162186</td>
      <td>0.187278</td>
      <td>0.054596</td>
      <td>-0.246167</td>
      <td>0.095576</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>Let's visualize this.</p>
<div class="highlight"><pre><span></span><span class="c1"># Generate a mask for the upper triangle</span>
<span class="n">corr_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">correlations</span><span class="p">)</span>
<span class="n">corr_mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">corr_mask</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Make the figsize 8x8</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="c1"># Plot heatmap of annotated correlations; change background to white</span>
<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">axes_style</span><span class="p">(</span><span class="s1">'white'</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">sort_corr</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> 
                <span class="n">cmap</span><span class="o">=</span><span class="s1">'RdBu_r'</span><span class="p">,</span> 
                <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">fmt</span><span class="o">=</span><span class="s1">'.0f'</span><span class="p">,</span>
                <span class="n">mask</span><span class="o">=</span><span class="n">corr_mask</span><span class="p">,</span>
                <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Correlations by Health Risk'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_149_0.png"></p>
<p>As expected, the result is similar to the correlations before data cleaning and feature engineering.</p>
<p>There are few obvious stronger correlations:</p>
<ul>
<li>
<p><code style="color:steelblue">waist</code> with <code style="color:steelblue">whr</code> and <code style="color:steelblue">bmi</code></p>
</li>
<li>
<p>blood presure measurements</p>
</li>
<li>
<p>cholesterol ratio and <code style="color:steelblue">chol</code> and <code style="color:steelblue">hdl</code></p>
</li>
</ul>
<p>But there are only few week correlations with <code style="color:steelblue">h_risk</code>:</p>
<ul>
<li><code style="color:steelblue">age</code>, <code style="color:steelblue">chol_ratio</code>, <code style="color:steelblue">waist</code></li>
</ul>
<h3>Pairplot for numeric features by health_risk</h3>
<p>Let's use Seaborn's <code>.pairplot()</code> function for additional analysis.</p>
<p>It plots only numerical features and I will use categorical feature of my target (<code style="color:steelblue">health_risk</code>) for coloring.</p>
<div class="highlight"><pre><span></span><span class="c1"># Plot Seaborn's pairplot</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'h_risk'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                 <span class="n">hue</span><span class="o">=</span><span class="s1">'health_risk'</span><span class="p">,</span>
                 <span class="n">palette</span><span class="o">=</span><span class="p">{</span><span class="s1">'diabetes'</span> <span class="p">:</span> <span class="s1">'coral'</span><span class="p">,</span> <span class="s1">'pre_diabetes'</span> <span class="p">:</span> <span class="s1">'palegreen'</span><span class="p">,</span>
                          <span class="s1">'no_diabetes'</span> <span class="p">:</span> <span class="s1">'dodgerblue'</span><span class="p">},</span>
                 <span class="n">plot_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">'alpha'</span> <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">'edgecolor'</span> <span class="p">:</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'linewidth'</span> <span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">fig</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">'Plot by Health Risk Classes'</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span>
             <span class="n">fontweight</span><span class="o">=</span><span class="s1">'bold'</span><span class="p">)</span>

<span class="c1"># save the plot for easier analyzing out of notebook</span>
<span class="n">g</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">'Figures/pairplot_health_risk.png'</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_153_0.png"></p>
<p>We have saved the plot so I could analyze it out of the notebook with any picture viewer.</p>
<p>At the first glance from the <em>scatter plots</em> I could notice few correlations like from the correlation heatmap.</p>
<p>Distributions of health risk classes is almost identical for each pair of features.</p>
<p>The <em>density plots</em> on the diagonal make it easier to compare these distributions. I can notice that only few pairs of features have slightly different distributions. For example, from the density plot for <code style="color:steelblue">age</code>, it could be seen that younger people has lees chance to have diabetes.</p>
<p>Let’s reduce the clutter by plotting only four features: three with the strongest correlations with health risk (<code style="color:steelblue">age</code>, <code style="color:steelblue">chol_ratio</code>, <code style="color:steelblue">waist</code>) and <code style="color:steelblue">bmi</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># Plot Seaborn's pairplot</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'h_risk'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                 <span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="s1">'age'</span><span class="p">,</span><span class="s1">'chol_ratio'</span><span class="p">,</span> <span class="s1">'waist'</span><span class="p">,</span> <span class="s1">'bmi'</span><span class="p">],</span> <span class="c1"># reduce to less features</span>
                 <span class="n">hue</span><span class="o">=</span><span class="s1">'health_risk'</span><span class="p">,</span>
                 <span class="n">palette</span><span class="o">=</span><span class="p">{</span><span class="s1">'diabetes'</span> <span class="p">:</span> <span class="s1">'coral'</span><span class="p">,</span> <span class="s1">'pre_diabetes'</span> <span class="p">:</span> <span class="s1">'palegreen'</span><span class="p">,</span>
                          <span class="s1">'no_diabetes'</span> <span class="p">:</span> <span class="s1">'dodgerblue'</span><span class="p">},</span>
                 <span class="n">plot_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">'alpha'</span> <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">'edgecolor'</span> <span class="p">:</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'linewidth'</span> <span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">fig</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">'Reduced Plot by Health Risk Classes'</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
             <span class="n">fontweight</span><span class="o">=</span><span class="s1">'bold'</span><span class="p">)</span>

<span class="c1"># save the plot</span>
<span class="n">g</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">'Figures/pairplot_health_risk_r.png'</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_156_0.png"></p>
<p>This looks much nicer! It is confirming the previous conclusions:</p>
<ul>
<li>There are very few and relatively weak correlations between health risk classes and other features</li>
</ul>
<p>This might cause problems for models to diferentiate among classes.</p>
<h3>Health risk vs. age and BMI</h3>
<p>First, let's use a groupby to show detailed statistics by class.</p>
<div class="highlight"><pre><span></span><span class="c1"># Segment by health_risk and display the means within each class</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'health_risk'</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">'h_risk'</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>age</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
      <th>h_risk</th>
    </tr>
    <tr>
      <th>health_risk</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>202.831034</td>
      <td>51.741379</td>
      <td>4.244483</td>
      <td>43.586207</td>
      <td>37.134483</td>
      <td>134.234483</td>
      <td>82.998276</td>
      <td>28.368621</td>
      <td>0.870724</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>206.541667</td>
      <td>47.125000</td>
      <td>4.916667</td>
      <td>56.666667</td>
      <td>40.125000</td>
      <td>141.437500</td>
      <td>80.625000</td>
      <td>29.141667</td>
      <td>0.922083</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>230.793651</td>
      <td>45.507937</td>
      <td>5.700000</td>
      <td>58.555556</td>
      <td>40.920635</td>
      <td>147.500000</td>
      <td>84.746032</td>
      <td>30.800000</td>
      <td>0.914286</td>
      <td>2.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Next, I will segment two key numeric features (<code style="color:steelblue">age</code> and <code style="color:steelblue">bmi</code>) by the targte variable <code style="color:steelblue">health_risk</code>.</p>
<p>We will use <strong>violin plots</strong> for that.</p>
<div class="highlight"><pre><span></span><span class="c1"># Segment age by health_risk and plot distributions</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s1">'health_risk'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">'age'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
            <span class="n">order</span><span class="o">=</span><span class="p">[</span><span class="s1">'no_diabetes'</span><span class="p">,</span> <span class="s1">'pre_diabetes'</span><span class="p">,</span><span class="s1">'diabetes'</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'health_risk - age'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Age'</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Health Risk'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_162_0.png"></p>
<p>The above plot is showing us that age is one of the key factors for diabetes risk. The age median and mean for risk classes are separated, but the variance is so high. That means that I have to look at many other factors that could contribute to higher risks for diabetes.</p>
<div class="highlight"><pre><span></span><span class="c1"># Segment bmi by health_risk and plot distributions</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s1">'health_risk'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">'bmi'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
            <span class="n">order</span><span class="o">=</span><span class="p">[</span><span class="s1">'no_diabetes'</span><span class="p">,</span> <span class="s1">'pre_diabetes'</span><span class="p">,</span><span class="s1">'diabetes'</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'health_risk - bmi'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Body Mass Index (BMI)'</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Health Risk'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_164_0.png"></p>
<p>Similar to the previous plot, higher bmi could lead to diabetes risk increase. But the variance is high and there is no strong direct correlation for that.</p>
<p>Let's now plot a bivariate segmentation for <code style="color:steelblue">bmi</code> and <code style="color:steelblue">age</code> segmented by <code style="color:steelblue">health_risk</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># Scatterplot of bmi vs. age</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'bmi'</span><span class="p">,</span>
           <span class="n">y</span><span class="o">=</span><span class="s1">'age'</span><span class="p">,</span>
           <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
           <span class="n">hue</span><span class="o">=</span><span class="s1">'health_risk'</span><span class="p">,</span>
           <span class="n">fit_reg</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
           <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">'alpha'</span><span class="p">:</span><span class="mf">0.8</span><span class="p">,</span> <span class="s1">'edgecolor'</span><span class="p">:</span><span class="s1">'k'</span><span class="p">,</span><span class="s1">'linewidth'</span><span class="p">:</span><span class="mf">0.5</span><span class="p">})</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'age - bmi'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Body Mass Index (BMI)'</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Age'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_166_0.png"></p>
<p>From this chart I can see less <code>diabetes</code> and <code>pre_diabetes</code> observations and more <code>no_diabetes</code> observations for lower <code style="color:steelblue">age</code> and lower <code style="color:steelblue">bmi</code>. </p>
<h3>Finalizing the dataset</h3>
<p>To finally finish data preparation I will:</p>
<ul>
<li>
<p>get dummy variables for both categorical features and create the new dataframe: <strong>abt</strong></p>
</li>
<li>
<p>drop categorical version (<code style="color:steelblue">health_risk</code>) of my target variable</p>
</li>
</ul>
<p><strong>Create dummy variables for all categorical features: <em>frame</em> and *gender</strong>*</p>
<div class="highlight"><pre><span></span><span class="c1"># Get dummy variables for frame and gender</span>
<span class="c1"># Create new dataframe with dummy features</span>
<span class="n">abt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'frame'</span><span class="p">,</span> <span class="s1">'gender'</span><span class="p">])</span>

<span class="n">abt</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>age</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
      <th>health_risk</th>
      <th>h_risk</th>
      <th>frame_Missing</th>
      <th>frame_large</th>
      <th>frame_medium</th>
      <th>frame_small</th>
      <th>gender_female</th>
      <th>gender_male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>46</td>
      <td>29.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
      <td>0.76</td>
      <td>no_diabetes</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>29</td>
      <td>46.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
      <td>0.96</td>
      <td>no_diabetes</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>58</td>
      <td>49.0</td>
      <td>187.5</td>
      <td>92.0</td>
      <td>48.4</td>
      <td>0.86</td>
      <td>no_diabetes</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>78.0</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>67</td>
      <td>33.0</td>
      <td>110.0</td>
      <td>50.0</td>
      <td>18.6</td>
      <td>0.87</td>
      <td>no_diabetes</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>249.0</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>64</td>
      <td>44.0</td>
      <td>138.0</td>
      <td>80.0</td>
      <td>27.8</td>
      <td>1.07</td>
      <td>diabetes</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Remove categorical version of my target variable</strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Drop health_risk feature</span>
<span class="n">abt</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'health_risk'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">abt</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>age</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
      <th>h_risk</th>
      <th>frame_Missing</th>
      <th>frame_large</th>
      <th>frame_medium</th>
      <th>frame_small</th>
      <th>gender_female</th>
      <th>gender_male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>46</td>
      <td>29.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
      <td>0.76</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>29</td>
      <td>46.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
      <td>0.96</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>58</td>
      <td>49.0</td>
      <td>187.5</td>
      <td>92.0</td>
      <td>48.4</td>
      <td>0.86</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>78.0</td>
      <td>12.0</td>
      <td>6.5</td>
      <td>67</td>
      <td>33.0</td>
      <td>110.0</td>
      <td>50.0</td>
      <td>18.6</td>
      <td>0.87</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>249.0</td>
      <td>28.0</td>
      <td>8.9</td>
      <td>64</td>
      <td>44.0</td>
      <td>138.0</td>
      <td>80.0</td>
      <td>27.8</td>
      <td>1.07</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Save the dataframe as the analytical base table</h3>
<div class="highlight"><pre><span></span><span class="c1"># Save analytical base table</span>
<span class="n">abt</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">'Data/analytical_base_table.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<h2>Algorithm Selection</h2>
<p>We have transformed the original dataset to the <strong>multi-class classification task</strong>.
The chosen algorithms are all known to work well in this kind of task:</p>
<ol>
<li>
<p>$L_1$-regularized Logistic Regression</p>
</li>
<li>
<p>$L_2$-regularized Logistic Regression</p>
</li>
<li>
<p>Support Vector Machine (SVM)</p>
</li>
<li>
<p>Random Forest</p>
</li>
<li>
<p>Gradient Boosting</p>
</li>
<li>
<p>AdaBoost</p>
</li>
</ol>
<p>Tree ensembles algorithms often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from all classes. This is one more reason to choose the last three algorithms.</p>
<p>We have tuned my models with ten-fold cross validation to obtain generalisation of accuracies and errors.</p>
<p><strong>Dictionary <code>'models'</code></strong></p>
<p>We are going to create the dictionary <code>models</code> with names of algorithms. I will be using it later for plotting and displaying results of the models.</p>
<div class="highlight"><pre><span></span><span class="c1"># Create models dictionary, it will be needed for ploting</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'l1'</span> <span class="p">:</span> <span class="s1">'Logistic Regression - L1'</span><span class="p">,</span>
    <span class="s1">'l2'</span> <span class="p">:</span> <span class="s1">'Logistic Regression - L2'</span><span class="p">,</span>
    <span class="s1">'svm'</span> <span class="p">:</span> <span class="s1">'Support Vector Machine (SVM)'</span><span class="p">,</span>
    <span class="s1">'rf'</span> <span class="p">:</span> <span class="s1">'Random Forest'</span><span class="p">,</span>
    <span class="s1">'gb'</span> <span class="p">:</span> <span class="s1">'Gradient Boosting'</span><span class="p">,</span>
    <span class="s1">'ab'</span> <span class="p">:</span> <span class="s1">'AdaBoost'</span>
<span class="p">}</span>
</pre></div>


<h4>Imbalanced Dataset</h4>
<p>As I discovered earlier, I are working with the <em>imbalanced classes</em>:</p>
<div class="highlight"><pre><span></span><span class="n">class_count</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">health_risk</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">health_risk</span><span class="o">:</span>     <span class="n">Count</span><span class="o">:</span>      <span class="o">%</span>
<span class="n">no_diabetes</span>      <span class="mi">290</span>         <span class="mf">76.92</span>
<span class="n">diabetes</span>     <span class="mi">63</span>          <span class="mf">16.71</span>
<span class="n">pre_diabetes</span>     <span class="mi">24</span>          <span class="mf">6.37</span>
</pre></div>


<p>As I are dealing with classification model, straight <strong>accuracy score</strong> is not the best option for measuring performance. Especially, it is not a good metric for evaluating <em>imbalanced classes</em> in the target variable.</p>
<p>The <strong>macro averaged f1 score</strong> metric is believed to be better option for the imbalanced classes. So, I will use it in:</p>
<ol>
<li>
<p><code>GridSearchCV()</code> as the scorer argument to take the performance metrics at each CV iteration (holdout folds)</p>
</li>
<li>
<p>the final evaluation of models to be compared with the accuracy score</p>
</li>
</ol>
<h4>Techniques for handling imbalanced classes</h4>
<p>We are going to train models in 4 groups. In each group, except the first one, I will use one of the techniques for handling imbalanced classes:</p>
<ul>
<li>
<p>A. Plain models (<strong>pm</strong>)</p>
</li>
<li>
<p>B. Macro averaged f1 score as a performance metric (<strong>f1</strong>)</p>
</li>
<li>
<p>C. Cost-Sensitive Learning with f1_macro (<strong>cs</strong>) </p>
</li>
<li>
<p>D. SMOTE - Synthetic Minority Over-sampling Technique with f1_macro (<strong>sm</strong>)</p>
</li>
</ul>
<h4>Variable 'target_names'</h4>
<p>Let's now set this variable. It will be used later for printing evaluation results.</p>
<div class="highlight"><pre><span></span><span class="n">target_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'no_diabetes'</span><span class="p">,</span> <span class="s1">'pre_diabetes'</span><span class="p">,</span> <span class="s1">'diabetes'</span><span class="p">]</span>
</pre></div>


<h2>Model Training</h2>
<p>Let's start by splitting my dataframe into separate objects: </p>
<ul>
<li>
<p><code style="color:steelblue">y</code> for the target varibale</p>
</li>
<li>
<p><code style="color:steelblue">X</code> for the input features</p>
</li>
</ul>
<p><strong>Separate dataframe into separate objects</strong></p>
<div class="highlight"><pre><span></span><span class="c1"># Display first 3 rows</span>
<span class="n">abt</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chol</th>
      <th>hdl</th>
      <th>chol_ratio</th>
      <th>age</th>
      <th>waist</th>
      <th>bp_s</th>
      <th>bp_d</th>
      <th>bmi</th>
      <th>whr</th>
      <th>h_risk</th>
      <th>frame_Missing</th>
      <th>frame_large</th>
      <th>frame_medium</th>
      <th>frame_small</th>
      <th>gender_female</th>
      <th>gender_male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>203.0</td>
      <td>56.0</td>
      <td>3.6</td>
      <td>46</td>
      <td>29.0</td>
      <td>118.0</td>
      <td>59.0</td>
      <td>22.1</td>
      <td>0.76</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>165.0</td>
      <td>24.0</td>
      <td>6.9</td>
      <td>29</td>
      <td>46.0</td>
      <td>112.0</td>
      <td>68.0</td>
      <td>37.4</td>
      <td>0.96</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>228.0</td>
      <td>37.0</td>
      <td>6.2</td>
      <td>58</td>
      <td>49.0</td>
      <td>187.5</td>
      <td>92.0</td>
      <td>48.4</td>
      <td>0.86</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="c1"># Object for target variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">abt</span><span class="o">.</span><span class="n">h_risk</span>

<span class="c1"># object for input features</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">abt</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'h_risk'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># display shapes of X and y</span>
<span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(377, 15) (377,)
</pre></div>


<p><strong>Split <code style="color:steelblue">X</code> and <code style="color:steelblue">y</code> into training and test sets</strong></p>
<p>We will continue with splitting my data into separate training and test sets.</p>
<ul>
<li>
<p>30% of observations will be set aside for the test set</p>
</li>
<li>
<p>the rest, 70%, will be used as the training set</p>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># Define random_state; I will use it through the notebook</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">789</span>

<span class="c1"># Split X and y into train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">abt</span><span class="o">.</span><span class="n">h_risk</span><span class="p">)</span>

<span class="c1"># Print number of observations in X_train, X_test, y_train, and y_test</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>263 114 263 114
</pre></div>


<h3>Helper Functions</h3>
<p>We will write several helper functions that will be used later in the project:</p>
<ol>
<li>
<p><strong>fit_tune_CV(<em>pipe, scorer</em>)</strong> - fit and tune models with cross validation</p>
</li>
<li>
<p><strong>evaluation(<em>fit_models</em>)</strong> - evaluate models (base score, f1_average, accuracy)</p>
</li>
<li>
<p><strong>eval_plot(<em>eval_df</em>)</strong> - plot evaluation metrics for all models</p>
</li>
<li>
<p><strong>plot_conf_mat_w_and_wo_norm(<em>fit_models, model_id, color</em>)</strong> - plot one model's confusion matrix heatmaps without and with normalization</p>
</li>
<li>
<p><strong>plot_norm_conf_matrices(<em>fit_models, color</em>)</strong> - plot normalized confusion matrix heatmaps for all fitted models</p>
</li>
<li>
<p><strong>class_rep_cm(<em>fit_models, model_id</em>)</strong> - display classification report and confusion matrix for one fitted model</p>
</li>
<li>
<p><strong>best_hyp_param(<em>fit_models</em>)</strong> - display best hyperparameters for all fitted models</p>
</li>
</ol>
<p><strong>1. Defining the function for fiting and tuning models with cross-validation</strong></p>
<pre>input: the pipeline and scorer
output: fitted models dictionary</pre>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_tune_CV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">scorer</span><span class="p">):</span>
    <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># Create empty dictionary called fitted_models</span>
    <span class="n">fit_models</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Loop through model pipelines, tuning each one and saving it to fitted_models</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">pipeline</span> <span class="ow">in</span> <span class="n">pipe</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Create cross-validation object from pipeline and hyperparameters</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="n">skf</span><span class="p">,</span>
                             <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Fit model on X_train, y_train</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Store model in fitted_models[name]</span>
        <span class="n">fit_models</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span>

        <span class="c1"># Print '{name} has been fitted'</span>
        <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">'has been fitted'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fit_models</span>
</pre></div>


<p><strong>2. The function for creating the dataframe with evaluation metrics for each model.</strong></p>
<pre>input: fitted models dcitionary
output: evaluation metrics dataframe</pre>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluation</span><span class="p">(</span><span class="n">fit_models</span><span class="p">):</span>
    <span class="n">lst</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">fit_models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">lst</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">name</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">),</span>
                    <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)])</span>

    <span class="n">eval_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lst</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'model'</span><span class="p">,</span> <span class="s1">'CV_score'</span><span class="p">,</span> <span class="s1">'f1_macro'</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">])</span>
    <span class="n">eval_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">'model'</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eval_df</span>
</pre></div>


<p><strong>3. The function for plotting evaluation metrics for each model.</strong></p>
<pre>input: dataframe with evaluation metrics
output: evaluation metrics plot</pre>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_plot</span><span class="p">(</span><span class="n">eval_df</span><span class="p">):</span>
    <span class="c1">#eval_df = evaluation(fit_models)</span>
    <span class="n">eval_dfp</span> <span class="o">=</span> <span class="n">eval_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="n">eval_dfp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">eval_dfp</span><span class="p">,</span><span class="n">id_vars</span><span class="o">=</span><span class="s1">'model'</span><span class="p">,</span><span class="n">var_name</span><span class="o">=</span><span class="s1">'metrics'</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s1">'score'</span><span class="p">)</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'model'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">'score'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">'metrics'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">eval_dfp</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">'bar'</span><span class="p">,</span>
                <span class="n">palette</span><span class="o">=</span><span class="p">{</span><span class="s1">'CV_score'</span> <span class="p">:</span> <span class="s1">'red'</span><span class="p">,</span> <span class="s1">'f1_macro'</span> <span class="p">:</span> <span class="s1">'orange'</span><span class="p">,</span>
                          <span class="s1">'accuracy'</span> <span class="p">:</span> <span class="s1">'royalblue'</span><span class="p">})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Evaluation Metrics'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Model'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Score'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><strong>4. The function for ploting one model's confusion matrix heatmaps without and with normalization.</strong></p>
<pre>input: fitted models dictionary, models dictionary key for one model, colormap for heatmaps
output: plot of two heatmaps</pre>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_conf_mat_w_and_wo_norm</span><span class="p">(</span><span class="n">fit_models</span><span class="p">,</span> <span class="n">model_id</span><span class="p">,</span> <span class="n">color</span><span class="p">):</span>
    <span class="c1"># Plot confusion matrix heatmaps</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">fit_models</span><span class="p">[</span><span class="n">model_id</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="n">model_id</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># confusion matrix without normalization</span>
    <span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span>
                <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">annot_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">),</span>
                <span class="n">fmt</span><span class="o">=</span><span class="s1">'d'</span><span class="p">,</span>
                <span class="n">cbar</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">target_names</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">target_names</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Confusion Matrix w/o Normalization'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Predicted Labels'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'True Labels'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

    <span class="c1"># normalized confusion matrix</span>
    <span class="n">matn</span> <span class="o">=</span> <span class="n">mat</span> <span class="o">/</span> <span class="n">mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">matn</span><span class="p">,</span>
                <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">annot_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">),</span>
                <span class="n">fmt</span><span class="o">=</span><span class="s1">'.2f'</span><span class="p">,</span>
                <span class="n">cbar</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                <span class="n">vmin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                <span class="n">vmax</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>

    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">target_names</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">target_names</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Normalized Confusion Matrix'</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Predicted Label'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'True Label'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><strong>5. The helper function for ploting heatmaps for normalized confusion matrices for all models.</strong></p>
<pre>input: fitted models dictionary and colormap for heatmaps
output: plot of heatmaps for each model</pre>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_norm_conf_matrices</span><span class="p">(</span><span class="n">fit_models</span><span class="p">,</span> <span class="n">color</span><span class="p">):</span>
    <span class="c1"># Prepare list of coordintaes for axes</span>
    <span class="n">lt</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">row</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fit_models</span><span class="p">)</span> <span class="o">/</span> <span class="n">col</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
            <span class="n">lt</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

    <span class="c1"># Create figure and subplots</span>
    <span class="n">figs_y</span> <span class="o">=</span> <span class="n">row</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">figs_y</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">'Normalized Confusion Matrices'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.94</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Loop for each fitted model        </span>
    <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">fit_models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">lt</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">lt</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>    
        <span class="c1"># normalized confusion matrix</span>
        <span class="n">matn</span> <span class="o">=</span> <span class="n">mat</span> <span class="o">/</span> <span class="n">mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">matn</span><span class="p">,</span>
                    <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                    <span class="n">annot_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">),</span>
                    <span class="n">fmt</span><span class="o">=</span><span class="s1">'.2f'</span><span class="p">,</span>
                    <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                    <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                    <span class="n">vmin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="n">vmax</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="c1">#cbar_kws = {'shrink' : 0.85},</span>
                    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

        <span class="n">axs</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">target_names</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">target_names</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Predicted Label'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'True Label'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><strong>6. The helper function for displaying confusion matrix and classification report.</strong></p>
<pre>input: fitted models dictionary and a models dictionary key for one of the models
output: confusion matrix dataframe and classification report</pre>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">class_rep_cm</span><span class="p">(</span><span class="n">fit_models</span><span class="p">,</span> <span class="n">model_id</span><span class="p">):</span>
    <span class="c1"># Predict classes using model_id</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">fit_models</span><span class="p">[</span><span class="n">model_id</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="n">models</span><span class="p">[</span><span class="n">model_id</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'='</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="n">model_id</span><span class="p">]))</span>

    <span class="c1"># Display confusion matrix for y_test and pred</span>
    <span class="n">conf_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">target_names</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">target_names</span><span class="p">)</span>
    <span class="n">conf_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">'True Labels'</span>
    <span class="n">conf_df</span> <span class="o">=</span> <span class="n">conf_df</span><span class="o">.</span><span class="n">rename_axis</span><span class="p">(</span><span class="s1">'Predicted Labels'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">'columns'</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">conf_df</span><span class="p">)</span>

    <span class="c1"># Display classification report</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>
</pre></div>


<p><strong>7. The helper function to display best_params_ for each fitted model.</strong></p>
<pre>input: fitted models dictionary
output: best_params_ dataframe</pre>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">best_hyp_param</span><span class="p">(</span><span class="n">fit_models</span><span class="p">):</span>
    <span class="c1"># Display best_params_ for each fitted model</span>

    <span class="c1"># Initialize empty dataframe</span>
    <span class="n">bp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1"># Loop through all fitted models</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">fit_models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Dictionary of best_params</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">best_params_</span>
        <span class="c1"># Model name from model dictionary</span>
        <span class="n">model_name</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

        <span class="c1"># Create dataframe for best_params_dictionary</span>
        <span class="n">bp_dft</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">'index'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Value'</span><span class="p">])</span>
        <span class="c1"># Insert the column 'Model'</span>
        <span class="n">bp_dft</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'Model'</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
        <span class="c1"># Concatenate previous dataframe with new one from this run</span>
        <span class="n">bp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">bp_df</span><span class="p">,</span> <span class="n">bp_dft</span><span class="p">])</span>

    <span class="c1"># Finalize the output of the dataframe</span>
    <span class="n">bp_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">bp_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">'Model'</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">bp_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">'index'</span> <span class="p">:</span> <span class="s1">'Hyperparameter'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bp_df</span>
</pre></div>


<h3>A. Plain models</h3>
<p>In this group I will train just plain models. The estimator's default scorer (if available) will be used.</p>
<h4>Build Model Pipelines</h4>
<p>Create a pipeline dictionary with pipelines for each algorithm</p>
<div class="highlight"><pre><span></span><span class="c1"># Pipeline dictionary</span>
<span class="n">pipelines_pm</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'l1'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                         <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l1'</span><span class="p">,</span><span class="n">solver</span><span class="o">=</span><span class="s1">'saga'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'l2'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                         <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'svm'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'rf'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'gb'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'ab'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>


<h3>Declare Hyperparameters Grids</h3>
<p>Next, I are going to declare hyperparameters to tune for all models.</p>
<p>These hyperparameters grids will be used in all model groups.</p>
<h4>Hyperparameter grids for  logistic regression</h4>
<div class="highlight"><pre><span></span><span class="c1"># Logistic Regression hyperparameters</span>
<span class="n">l1_hyperparameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'logisticregression__C'</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">l2_hyperparameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'logisticregression__C'</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="s1">'logisticregression__solver'</span> <span class="p">:</span> <span class="p">[</span><span class="s1">'newton-cg'</span><span class="p">,</span> <span class="s1">'lbfgs'</span><span class="p">,</span> <span class="s1">'sag'</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>


<h4>Hyperparameter grid for SVM</h4>
<div class="highlight"><pre><span></span><span class="c1"># SVM hyperparameters</span>
<span class="n">svm_hyperparameters</span> <span class="o">=</span> <span class="p">{</span> 
    <span class="s1">'svc__kernel'</span> <span class="p">:</span> <span class="p">[</span><span class="s1">'linear'</span><span class="p">,</span> <span class="s1">'rbf'</span><span class="p">,</span> <span class="s1">'poly'</span><span class="p">,</span> <span class="s1">'sigmoid'</span><span class="p">],</span>
    <span class="s1">'svc__C'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0005</span><span class="p">,</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="s1">'svc__gamma'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>


<h4>Hyperparameter grid for random forest</h4>
<div class="highlight"><pre><span></span><span class="c1"># Random Forest hyperparameters</span>
<span class="n">rf_hyperparameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'randomforestclassifier__n_estimators'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">],</span>
    <span class="s1">'randomforestclassifier__max_features'</span> <span class="p">:</span> <span class="p">[</span><span class="s1">'sqrt'</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">],</span>
    <span class="s1">'randomforestclassifier__min_samples_leaf'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span> <span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span>
    <span class="s1">'randomforestclassifier__criterion'</span> <span class="p">:</span> <span class="p">[</span><span class="s1">'gini'</span><span class="p">,</span> <span class="s1">'entropy'</span><span class="p">],</span>
    <span class="s1">'randomforestclassifier__min_samples_split'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>


<h4>Hyperparameter grid for the boosted tree</h4>
<div class="highlight"><pre><span></span><span class="c1"># Boosted Tree hyperparameters</span>
<span class="n">gb_hyperparameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'gradientboostingclassifier__n_estimators'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>
    <span class="s1">'gradientboostingclassifier__learning_rate'</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="s1">'gradientboostingclassifier__max_depth'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>


<h4>Hyperparameter grid for AdaBoost</h4>
<div class="highlight"><pre><span></span><span class="c1"># AdaBoost hyperparameters</span>
<span class="n">ab_hyperparameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'adaboostclassifier__n_estimators'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="s1">'adaboostclassifier__learning_rate'</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>


<h4>Store all declared hyperparamters in a dictionary</h4>
<p>Finally, I will store all my declared hyperparameters in a dictionary <code style="color:steelblue">hyperparameters</code>.</p>
<p>We are going to use the same keys as in the <code style="color:steelblue">pipelines</code> dictionary.</p>
<div class="highlight"><pre><span></span><span class="c1"># Create hyperparameters dictionary</span>
<span class="n">hyperparameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'l1'</span> <span class="p">:</span> <span class="n">l1_hyperparameters</span><span class="p">,</span>
    <span class="s1">'l2'</span> <span class="p">:</span> <span class="n">l2_hyperparameters</span><span class="p">,</span>
    <span class="s1">'svm'</span> <span class="p">:</span> <span class="n">svm_hyperparameters</span><span class="p">,</span>
    <span class="s1">'rf'</span> <span class="p">:</span> <span class="n">rf_hyperparameters</span><span class="p">,</span>
    <span class="s1">'gb'</span> <span class="p">:</span> <span class="n">gb_hyperparameters</span><span class="p">,</span>
    <span class="s1">'ab'</span> <span class="p">:</span> <span class="n">ab_hyperparameters</span>
<span class="p">}</span>
</pre></div>


<h3>Fit and Tune Models with Cross-Validation</h3>
<p>We will run the function <code>fit_tune_CV</code> to fit and tune with cross validation all models</p>
<div class="highlight"><pre><span></span><span class="c1"># Fit and tune models with cross-validation</span>
<span class="n">fitted_models_pm</span> <span class="o">=</span> <span class="n">fit_tune_CV</span><span class="p">(</span><span class="n">pipelines_pm</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>l1 has been fitted
l2 has been fitted
svm has been fitted
rf has been fitted
gb has been fitted
ab has been fitted
</pre></div>


<h3>Display evaluation metrics</h3>
<p>Let's now check the metrics for my models:</p>
<div class="highlight"><pre><span></span><span class="c1"># Plot and display evaluation metrics for simple plain models</span>
<span class="n">eval_df_pm</span> <span class="o">=</span> <span class="n">evaluation</span><span class="p">(</span><span class="n">fitted_models_pm</span><span class="p">)</span>
<span class="n">eval_plot</span><span class="p">(</span><span class="n">eval_df_pm</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">eval_df_pm</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_238_0.png"></p>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV_score</th>
      <th>f1_macro</th>
      <th>accuracy</th>
    </tr>
    <tr>
      <th>model</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>l1</th>
      <td>0.768061</td>
      <td>0.290429</td>
      <td>0.771930</td>
    </tr>
    <tr>
      <th>l2</th>
      <td>0.768061</td>
      <td>0.290429</td>
      <td>0.771930</td>
    </tr>
    <tr>
      <th>svm</th>
      <td>0.771863</td>
      <td>0.290429</td>
      <td>0.771930</td>
    </tr>
    <tr>
      <th>rf</th>
      <td>0.787072</td>
      <td>0.410081</td>
      <td>0.789474</td>
    </tr>
    <tr>
      <th>gb</th>
      <td>0.768061</td>
      <td>0.290429</td>
      <td>0.771930</td>
    </tr>
    <tr>
      <th>ab</th>
      <td>0.771863</td>
      <td>0.380148</td>
      <td>0.763158</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
<li>
<p>In this group of models <strong>Random Forest</strong> has the best all scores.</p>
</li>
<li>
<p>CV_scores are holdout accuracy scores. For classification tasks the default scoring metric is accuracy.</p>
</li>
<li>
<p>Models were tuned to have the best accuracy score and this is why f1_macro scores are very low.</p>
</li>
</ul>
<p>We are going to check now confusion matrices and classification reports to confirm that..</p>
<p><strong>Plot confusion matrix heatmaps for one model.</strong></p>
<p>Let's compare:</p>
<ol>
<li>
<p>Confusion matrix without normalization (left)</p>
</li>
<li>
<p>Confusion matrix with normalization (right)</p>
</li>
</ol>
<div class="highlight"><pre><span></span><span class="c1"># Plot confusion matrix heatmaps without and with normalization</span>
<span class="n">plot_conf_mat_w_and_wo_norm</span><span class="p">(</span><span class="n">fitted_models_pm</span><span class="p">,</span> <span class="s1">'rf'</span><span class="p">,</span> <span class="s1">'Reds'</span><span class="p">)</span>                          
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_241_0.png"></p>
<p><strong>A few notes about above heatmaps:</strong></p>
<ul>
<li>
<p>The diagonal elements in the confusion matrix represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier.</p>
</li>
<li>
<p>The higher the diagonal values of the confusion matrix the better, indicating many correct predictions</p>
</li>
<li>
<p>Due to heavy class imbalance, I should perform normalization by number of elements in each class to have a more visual interpretation of which class is being misclassified.</p>
</li>
<li>
<p>For the normalized confusion matrix, the goal is to have darker (closer to 1.0) diagonal elments and brighter (closer to 0.0) off-diagonal elements.</p>
</li>
<li>
<p>In my case it is obvious that lack of samples for two minority classes (<code>'diabetes'</code>, <code>'pre_diabetes'</code>) caused their misplacements</p>
</li>
</ul>
<h3>Plot normalized confusion matrix heatmaps for all fitted models</h3>
<div class="highlight"><pre><span></span><span class="c1"># Plot all confusion matrices for simple and plain models</span>
<span class="n">plot_norm_conf_matrices</span><span class="p">(</span><span class="n">fitted_models_pm</span><span class="p">,</span> <span class="s1">'Reds'</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_244_0.png"></p>
<ul>
<li>
<p>It looks like the modules are predicting the majority class (<code>no_diabetes</code>) only. The recall for this class is 1.00 except for Random Forest (0.97) and AdaBoost (0.94).</p>
</li>
<li>
<p>None of the modules predicted any label in the <code>pre_diabetes</code> class.</p>
</li>
<li>
<p>Only Random Forest and AdaBoost predicted a few points in the <code>diabetes</code> class.</p>
</li>
</ul>
<h3>Display confusion matrix and classification report for one or more models</h3>
<p>Let's compare Random Forest and AdaBoost performances.</p>
<div class="highlight"><pre><span></span><span class="c1"># Display classification report and confusion matrix</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_pm</span><span class="p">,</span> <span class="s1">'rf'</span><span class="p">)</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_pm</span><span class="p">,</span> <span class="s1">'ab'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>     Random Forest
     =============
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>85</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>5</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>14</td>
      <td>0</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.82      0.97      0.89        88
pre_diabetes       0.00      0.00      0.00         7
    diabetes       0.50      0.26      0.34        19

   micro avg       0.79      0.79      0.79       114
   macro avg       0.44      0.41      0.41       114
weighted avg       0.71      0.79      0.74       114


     AdaBoost
     ========
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>83</td>
      <td>0</td>
      <td>5</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>6</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>15</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.80      0.94      0.86        88
pre_diabetes       0.00      0.00      0.00         7
    diabetes       0.40      0.21      0.28        19

   micro avg       0.76      0.76      0.76       114
   macro avg       0.40      0.38      0.38       114
weighted avg       0.68      0.76      0.71       114
</pre></div>


<ul>
<li>
<p>As I saw before, Random Forest has slightly better f1_macro score than AdaBoost.</p>
</li>
<li>
<p>It is a winner for this group.</p>
</li>
</ul>
<h3>Display best_params_ for all fitted models</h3>
<p>Let's list the best hyperparameters for all fitted models.</p>
<div class="highlight"><pre><span></span><span class="c1"># Display best_params_ for all fitted models</span>
<span class="n">best_hyp_param</span><span class="p">(</span><span class="n">fitted_models_pm</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Hyperparameter</th>
      <th>Value</th>
    </tr>
    <tr>
      <th>Model</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Logistic Regression - L1</th>
      <td>logisticregression__C</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__C</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__solver</td>
      <td>newton-cg</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__C</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__gamma</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__kernel</td>
      <td>poly</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__criterion</td>
      <td>gini</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__max_features</td>
      <td>0.33</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_leaf</td>
      <td>5</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_split</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__n_estimators</td>
      <td>100</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__learning_rate</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__max_depth</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__n_estimators</td>
      <td>100</td>
    </tr>
    <tr>
      <th>AdaBoost</th>
      <td>adaboostclassifier__learning_rate</td>
      <td>0.05</td>
    </tr>
    <tr>
      <th>AdaBoost</th>
      <td>adaboostclassifier__n_estimators</td>
      <td>100</td>
    </tr>
  </tbody>
</table>
</div>

<h3>B. F1-macro score</h3>
<p>In this group I will run just plain models with only one change. I are going  to change the performance metric for evalutating models. </p>
<p>The <strong>macro averaged f1 score metric</strong> will be used as the scorer argument in GridSearchCV() and in the final evaluation of the models. It is better option for imbalanced classes than the 
accuracy score.</p>
<h4>Build Model Pipelines</h4>
<p>Create a pipeline dictionary with pipelines for each algorithm</p>
<div class="highlight"><pre><span></span><span class="c1"># Pipeline dictionary</span>
<span class="n">pipelines_f1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'l1'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                         <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l1'</span><span class="p">,</span><span class="n">solver</span><span class="o">=</span><span class="s1">'saga'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'l2'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                         <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'svm'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'rf'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'gb'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'ab'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>


<h3>Fit and Tune Models with Cross-Validation</h3>
<p>We will run the function <code>fit_tune_CV</code> to fit and tune with cross validation all models</p>
<div class="highlight"><pre><span></span><span class="c1"># Fit and tune models with cross-validation</span>
<span class="n">fitted_models_f1</span> <span class="o">=</span> <span class="n">fit_tune_CV</span><span class="p">(</span><span class="n">pipelines_f1</span><span class="p">,</span> <span class="s1">'f1_macro'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>l1 has been fitted
l2 has been fitted
svm has been fitted
rf has been fitted
gb has been fitted
ab has been fitted
</pre></div>


<h3>Display evaluation metrics</h3>
<p>Let's now check the metrics for my models:</p>
<div class="highlight"><pre><span></span><span class="c1"># Plot and display evaluation metrics for simple plain models</span>
<span class="n">eval_df_f1</span> <span class="o">=</span> <span class="n">evaluation</span><span class="p">(</span><span class="n">fitted_models_f1</span><span class="p">)</span>
<span class="n">eval_plot</span><span class="p">(</span><span class="n">eval_df_f1</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">eval_df_f1</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_263_0.png"></p>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV_score</th>
      <th>f1_macro</th>
      <th>accuracy</th>
    </tr>
    <tr>
      <th>model</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>l1</th>
      <td>0.376153</td>
      <td>0.374365</td>
      <td>0.780702</td>
    </tr>
    <tr>
      <th>l2</th>
      <td>0.377366</td>
      <td>0.369116</td>
      <td>0.771930</td>
    </tr>
    <tr>
      <th>svm</th>
      <td>0.364190</td>
      <td>0.351267</td>
      <td>0.719298</td>
    </tr>
    <tr>
      <th>rf</th>
      <td>0.405695</td>
      <td>0.473797</td>
      <td>0.824561</td>
    </tr>
    <tr>
      <th>gb</th>
      <td>0.427936</td>
      <td>0.513450</td>
      <td>0.771930</td>
    </tr>
    <tr>
      <th>ab</th>
      <td>0.395413</td>
      <td>0.505135</td>
      <td>0.789474</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
<li>
<p>In this group of models <strong>Gradient Boosting</strong> has the best f1 macro score (0.513450) and the CV-score (0.427936) as well.</p>
</li>
<li>
<p><strong>Random Forest</strong> has the best accuracy but all models have much higher accuracy because they are mostly predicting the majority class (<code>no_diabetes</code>).</p>
</li>
</ul>
<p>We are going to check now confusion matrices and classification reports to see more details.</p>
<h3>Plot normalized confusion matrix heatmaps for all fitted models</h3>
<div class="highlight"><pre><span></span><span class="c1"># Plot all confusion matrices for simple and plain models</span>
<span class="n">plot_norm_conf_matrices</span><span class="p">(</span><span class="n">fitted_models_f1</span><span class="p">,</span> <span class="s1">'Oranges'</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_266_0.png"></p>
<p>The Gradient Boosting confusion matrix has the darkest diagonal elements. This is a visual indication of the model performance.</p>
<p>As it was expected, tree ensembles algorithms are performing best on my imbalanced dataset.</p>
<p>All models are having problems with minority classes, especially with the smallest one.</p>
<h3>Display confusion matrix and classification report for one or more models</h3>
<p>Let's compare Gradient Boost and Random Forest performances.</p>
<div class="highlight"><pre><span></span><span class="c1"># Display classification report and confusion matrix</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_f1</span><span class="p">,</span> <span class="s1">'gb'</span><span class="p">)</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_f1</span><span class="p">,</span> <span class="s1">'rf'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>     Gradient Boosting
     =================
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>78</td>
      <td>1</td>
      <td>9</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>5</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>9</td>
      <td>1</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.85      0.89      0.87        88
pre_diabetes       0.33      0.14      0.20         7
    diabetes       0.47      0.47      0.47        19

   micro avg       0.77      0.77      0.77       114
   macro avg       0.55      0.50      0.51       114
weighted avg       0.75      0.77      0.76       114


     Random Forest
     =============
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>86</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>5</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>11</td>
      <td>0</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.84      0.98      0.91        88
pre_diabetes       0.00      0.00      0.00         7
    diabetes       0.67      0.42      0.52        19

   micro avg       0.82      0.82      0.82       114
   macro avg       0.50      0.47      0.47       114
weighted avg       0.76      0.82      0.78       114
</pre></div>


<ul>
<li>
<p>As I saw before, Random Forest mostly predicts the majority class, recall=0.98, but it has 0 recall for the <code>pre_diabetes</code> class. </p>
</li>
<li>
<p>Gradient Boost is doing slightly better with minority classes and that makes it the winner of this group.</p>
</li>
</ul>
<h3>Display best_params_ for all fitted models</h3>
<p>Let's list the best hyperparameters for all fitted models.</p>
<div class="highlight"><pre><span></span><span class="c1"># Display best_params_ for all fitted models</span>
<span class="n">best_hyp_param</span><span class="p">(</span><span class="n">fitted_models_f1</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Hyperparameter</th>
      <th>Value</th>
    </tr>
    <tr>
      <th>Model</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Logistic Regression - L1</th>
      <td>logisticregression__C</td>
      <td>0.5</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__C</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__solver</td>
      <td>newton-cg</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__C</td>
      <td>0.5</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__gamma</td>
      <td>5</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__kernel</td>
      <td>sigmoid</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__criterion</td>
      <td>gini</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__max_features</td>
      <td>0.33</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_leaf</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_split</td>
      <td>3</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__n_estimators</td>
      <td>50</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__learning_rate</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__max_depth</td>
      <td>3</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__n_estimators</td>
      <td>300</td>
    </tr>
    <tr>
      <th>AdaBoost</th>
      <td>adaboostclassifier__learning_rate</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>AdaBoost</th>
      <td>adaboostclassifier__n_estimators</td>
      <td>100</td>
    </tr>
  </tbody>
</table>
</div>

<h3>C. Cost-Sensitive Learning</h3>
<p>In this group I will use <strong>cost-sensitive learning (penalize algorithms)</strong>. They increase the cost of classification mistakes on the minority class.</p>
<p>Cost-sensitive learning is not supported by boosting algorithms so I will have 4 models in this group.</p>
<p>The argument <code>class_weight='balanced'</code> is used during training to penalize mistakes on the minority classes by an amount proportional to how under-represented they are.</p>
<p>We will do this slightly differently. First I will use compute_class_weight to calculate <code>'balanced'</code> class weights and then I will pass them to classifiers. This way I can print calculated weight classes:</p>
<pre>0: 0.43
1: 5.16
2: 1.99
</pre>

<p>Compare this with the population of classes:
</p><pre>0: 290
1: 24
2: 63
</pre><p></p>
<div class="highlight"><pre><span></span><span class="c1"># Define dictionary with class weights</span>
<span class="n">class_weight_list</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="s1">'balanced'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">class_weights</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="n">class_weight_list</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">class_weights</span><span class="p">)</span>

<span class="c1"># count population of classes</span>
<span class="k">print</span><span class="p">(</span><span class="n">Counter</span><span class="p">(</span><span class="n">abt</span><span class="o">.</span><span class="n">h_risk</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>{0: 0.43399339933993397, 1: 5.1568627450980395, 2: 1.9924242424242424}
Counter({0: 290, 2: 63, 1: 24})
</pre></div>


<h4>Build Model Pipelines</h4>
<p>Create a pipeline dictionary with pipelines for each algorithm. </p>
<p>Pass the class weights dictionary to the <code>class_weight</code> argument in classifiers.</p>
<div class="highlight"><pre><span></span><span class="c1"># Pipeline dictionary for class_weight</span>
<span class="n">pipelines_cs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'l1'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                         <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l1'</span><span class="p">,</span><span class="n">solver</span><span class="o">=</span><span class="s1">'saga'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'l2'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                         <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'svm'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                          <span class="n">SVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'rf'</span> <span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                         <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>


<h3>Fit and Tune Models with Cross-Validation</h3>
<div class="highlight"><pre><span></span><span class="c1"># Fit and tune models with cross-validation</span>
<span class="n">fitted_models_cs</span> <span class="o">=</span> <span class="n">fit_tune_CV</span><span class="p">(</span><span class="n">pipelines_cs</span><span class="p">,</span> <span class="s1">'f1_macro'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>l1 has been fitted
l2 has been fitted
svm has been fitted
rf has been fitted
</pre></div>


<h3>Display evaluation metrics</h3>
<div class="highlight"><pre><span></span><span class="c1"># Plot and display evaluation metrics for simple plain models</span>
<span class="n">eval_df_cs</span> <span class="o">=</span> <span class="n">evaluation</span><span class="p">(</span><span class="n">fitted_models_cs</span><span class="p">)</span>
<span class="n">eval_plot</span><span class="p">(</span><span class="n">eval_df_cs</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">eval_df_cs</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_284_0.png"></p>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV_score</th>
      <th>f1_macro</th>
      <th>accuracy</th>
    </tr>
    <tr>
      <th>model</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>l1</th>
      <td>0.436358</td>
      <td>0.385859</td>
      <td>0.622807</td>
    </tr>
    <tr>
      <th>l2</th>
      <td>0.414028</td>
      <td>0.455797</td>
      <td>0.631579</td>
    </tr>
    <tr>
      <th>svm</th>
      <td>0.401227</td>
      <td>0.432931</td>
      <td>0.728070</td>
    </tr>
    <tr>
      <th>rf</th>
      <td>0.438159</td>
      <td>0.556052</td>
      <td>0.754386</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
<li>In this group Random Forest has the best all scores and most importantly f1_macro (0.556052).</li>
</ul>
<p>Let's analyze confusion matrices and classification reports.</p>
<h3>Plot normalized confusion matrix heatmaps for all fitted models</h3>
<div class="highlight"><pre><span></span><span class="c1"># Plot all confusion matrices for simple and plain models</span>
<span class="n">plot_norm_conf_matrices</span><span class="p">(</span><span class="n">fitted_models_cs</span><span class="p">,</span> <span class="s1">'Greens'</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_287_0.png"></p>
<ul>
<li>
<p>All models were able to predict at least one point for the minority classes. Nice.</p>
</li>
<li>
<p>Random Forest did good on <code>no_diabetes</code> and <code>diabetes</code> classes, but Logistic Regression - L2 was the best with predicting <code>pre_diabetes</code>.</p>
</li>
</ul>
<h3>Display confusion matrix and classification report for one or more models</h3>
<p>Let's compare Random Forest and Logistic Regression - L2.</p>
<div class="highlight"><pre><span></span><span class="c1"># Display classification report and confusion matrix</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_cs</span><span class="p">,</span> <span class="s1">'rf'</span><span class="p">)</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_cs</span><span class="p">,</span> <span class="s1">'l2'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>     Random Forest
     =============
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>73</td>
      <td>3</td>
      <td>12</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>4</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>7</td>
      <td>1</td>
      <td>11</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.87      0.83      0.85        88
pre_diabetes       0.33      0.29      0.31         7
    diabetes       0.46      0.58      0.51        19

   micro avg       0.75      0.75      0.75       114
   macro avg       0.55      0.56      0.56       114
weighted avg       0.77      0.75      0.76       114


     Logistic Regression - L2
     ========================
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>61</td>
      <td>12</td>
      <td>15</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>4</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>5</td>
      <td>6</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.87      0.69      0.77        88
pre_diabetes       0.14      0.43      0.21         7
    diabetes       0.35      0.42      0.38        19

   micro avg       0.63      0.63      0.63       114
   macro avg       0.45      0.51      0.46       114
weighted avg       0.74      0.63      0.67       114
</pre></div>


<ul>
<li>
<p>As I mentioned before, Logistic Regression - L2 did good on pre_diabetes, recall=0.43, but did relatively bad on the majority class.</p>
</li>
<li>
<p>Random Forest did well on diabetes and pre_diabetes (recall=0.58) but slightly worse on pre_diabetes. It is the winner in this group.</p>
</li>
</ul>
<h3>Display best_params_ for all fitted models</h3>
<div class="highlight"><pre><span></span><span class="c1"># Display best_params_ for all fitted models</span>
<span class="n">best_hyp_param</span><span class="p">(</span><span class="n">fitted_models_cs</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Hyperparameter</th>
      <th>Value</th>
    </tr>
    <tr>
      <th>Model</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Logistic Regression - L1</th>
      <td>logisticregression__C</td>
      <td>0.05</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__C</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__solver</td>
      <td>sag</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__C</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__gamma</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__kernel</td>
      <td>poly</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__criterion</td>
      <td>entropy</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__max_features</td>
      <td>0.33</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_leaf</td>
      <td>5</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_split</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__n_estimators</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div>

<h3>D. SMOTE (Synthetic Minority Over-sampling Technique)</h3>
<p>For this group I will utilize SMOTE to handle issues with the imbalanced data.</p>
<p>SMOTE is an over-sampling method that creates new (synthetic) samples based on the samples in my minority classes. It finds the k-nearest-neighbors of each member of the minority classes.</p>
<p>The new samples should be generated only in the training set to ensure my model generalizes well to unseen data.</p>
<p>We will be using <a href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html">imblearn</a> python package.</p>
<h4>Build Model Pipelines</h4>
<ul>
<li>
<p>I are going to use the Pipeline from the imblearn package in place of scikit-learn Pipeline. </p>
</li>
<li>
<p>It takes care automatically to re-sample when called fit() on the pipeline, and does not re-sample test data (when called transform() or predict()).</p>
</li>
</ul>
<p>Create a pipeline dictionary with pipelines for each algorithm.</p>
<div class="highlight"><pre><span></span><span class="c1"># Pipeline dictionary for SMOTE</span>
<span class="n">pipelines_sm</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'l1'</span> <span class="p">:</span> <span class="n">imbl_pipe</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">),</span>
                     <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l1'</span><span class="p">,</span><span class="n">solver</span><span class="o">=</span><span class="s1">'saga'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'l2'</span> <span class="p">:</span> <span class="n">imbl_pipe</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">),</span>
                     <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">'multinomial'</span><span class="p">,</span>
                                        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'svm'</span> <span class="p">:</span> <span class="n">imbl_pipe</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">),</span>
                      <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'rf'</span> <span class="p">:</span> <span class="n">imbl_pipe</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">),</span>
                     <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'gb'</span> <span class="p">:</span> <span class="n">imbl_pipe</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">),</span>
                     <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)),</span>
    <span class="s1">'ab'</span> <span class="p">:</span> <span class="n">imbl_pipe</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">),</span>
                     <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>


<h3>Fit and Tune Models with Cross-Validation</h3>
<div class="highlight"><pre><span></span><span class="c1"># Fit and tune models with cross-validation</span>
<span class="n">fitted_models_sm</span> <span class="o">=</span> <span class="n">fit_tune_CV</span><span class="p">(</span><span class="n">pipelines_sm</span><span class="p">,</span> <span class="s1">'f1_macro'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>l1 has been fitted
l2 has been fitted
svm has been fitted
rf has been fitted
gb has been fitted
ab has been fitted
</pre></div>


<h3>Display evaluation metrics</h3>
<div class="highlight"><pre><span></span><span class="c1"># Plot and display evaluation metrics for simple plain models</span>
<span class="n">eval_df_sm</span> <span class="o">=</span> <span class="n">evaluation</span><span class="p">(</span><span class="n">fitted_models_sm</span><span class="p">)</span>
<span class="n">eval_plot</span><span class="p">(</span><span class="n">eval_df_sm</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">eval_df_sm</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_303_0.png"></p>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV_score</th>
      <th>f1_macro</th>
      <th>accuracy</th>
    </tr>
    <tr>
      <th>model</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>l1</th>
      <td>0.423257</td>
      <td>0.378132</td>
      <td>0.605263</td>
    </tr>
    <tr>
      <th>l2</th>
      <td>0.408130</td>
      <td>0.434748</td>
      <td>0.622807</td>
    </tr>
    <tr>
      <th>svm</th>
      <td>0.417366</td>
      <td>0.405540</td>
      <td>0.605263</td>
    </tr>
    <tr>
      <th>rf</th>
      <td>0.458407</td>
      <td>0.511111</td>
      <td>0.719298</td>
    </tr>
    <tr>
      <th>gb</th>
      <td>0.439936</td>
      <td>0.522429</td>
      <td>0.736842</td>
    </tr>
    <tr>
      <th>ab</th>
      <td>0.415525</td>
      <td>0.437384</td>
      <td>0.640351</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
<li>
<p>In this group Random Forest has the best cross-validation score.</p>
</li>
<li>
<p>But, Gradient Boost performs the best on the test set, both accuracy and f1-macro score.</p>
</li>
</ul>
<p>Let's now check confusion matrices and classification reports.</p>
<h3>Plot normalized confusion matrix heatmaps for all fitted models</h3>
<div class="highlight"><pre><span></span><span class="c1"># Plot all confusion matrices for simple and plain models</span>
<span class="n">plot_norm_conf_matrices</span><span class="p">(</span><span class="n">fitted_models_sm</span><span class="p">,</span> <span class="s1">'Purples'</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_306_0.png"></p>
<ul>
<li>
<p>From confusion matrices looks like that Random Forest outperforms Gradient Boost. It is even or better on minority classes but slightly worse on the majority class.</p>
</li>
<li>
<p>From calssification reports below I can see that Random Forest is doing bad on precison for the <code>pre-diabetes</code> class. 0.15 compared with 0.25 precision of Gradient Boost. That makes f1-macro worst.</p>
</li>
</ul>
<h3>Display confusion matrix and classification report for one or more models</h3>
<div class="highlight"><pre><span></span><span class="c1"># Display classification report and confusion matrix</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_sm</span><span class="p">,</span> <span class="s1">'gb'</span><span class="p">)</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_sm</span><span class="p">,</span> <span class="s1">'rf'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>     Gradient Boosting
     =================
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>73</td>
      <td>5</td>
      <td>10</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>4</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>9</td>
      <td>1</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.85      0.83      0.84        88
pre_diabetes       0.25      0.29      0.27         7
    diabetes       0.45      0.47      0.46        19

   micro avg       0.74      0.74      0.74       114
   macro avg       0.52      0.53      0.52       114
weighted avg       0.75      0.74      0.74       114


     Random Forest
     =============
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>70</td>
      <td>8</td>
      <td>10</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>4</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>6</td>
      <td>3</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.88      0.80      0.83        88
pre_diabetes       0.15      0.29      0.20         7
    diabetes       0.48      0.53      0.50        19

   micro avg       0.72      0.72      0.72       114
   macro avg       0.50      0.54      0.51       114
weighted avg       0.76      0.72      0.74       114
</pre></div>


<h3>Display best_params_ for all fitted models</h3>
<div class="highlight"><pre><span></span><span class="c1"># Display best_params_ for all fitted models</span>
<span class="n">best_hyp_param</span><span class="p">(</span><span class="n">fitted_models_sm</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Hyperparameter</th>
      <th>Value</th>
    </tr>
    <tr>
      <th>Model</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Logistic Regression - L1</th>
      <td>logisticregression__C</td>
      <td>0.05</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__C</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>Logistic Regression - L2</th>
      <td>logisticregression__solver</td>
      <td>newton-cg</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__C</td>
      <td>0.5</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__gamma</td>
      <td>5</td>
    </tr>
    <tr>
      <th>Support Vector Machine (SVM)</th>
      <td>svc__kernel</td>
      <td>linear</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__criterion</td>
      <td>gini</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__max_features</td>
      <td>0.33</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_leaf</td>
      <td>15</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__min_samples_split</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>randomforestclassifier__n_estimators</td>
      <td>100</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__learning_rate</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__max_depth</td>
      <td>4</td>
    </tr>
    <tr>
      <th>Gradient Boosting</th>
      <td>gradientboostingclassifier__n_estimators</td>
      <td>200</td>
    </tr>
    <tr>
      <th>AdaBoost</th>
      <td>adaboostclassifier__learning_rate</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>AdaBoost</th>
      <td>adaboostclassifier__n_estimators</td>
      <td>200</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Winner</h3>
<p>Let's collect my evalutaion results into one dataframe.</p>
<div class="highlight"><pre><span></span><span class="c1"># Create dataframe with evaluations for all modules</span>

<span class="c1"># initialize dataframe</span>
<span class="n">eval_df_all</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="c1"># dictionary of evaluation dataframes</span>
<span class="n">dgroups</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'pm'</span> <span class="p">:</span> <span class="n">eval_df_pm</span><span class="p">,</span> <span class="s1">'f1'</span><span class="p">:</span><span class="n">eval_df_f1</span><span class="p">,</span> <span class="s1">'cs'</span><span class="p">:</span><span class="n">eval_df_cs</span><span class="p">,</span> <span class="s1">'sm'</span><span class="p">:</span><span class="n">eval_df_sm</span><span class="p">}</span>
<span class="c1"># list of model groups abreviations </span>
<span class="n">mgroups</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dgroups</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="c1"># concatenate all dataframes</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">mgroups</span><span class="p">:</span>
    <span class="n">eval_df_t</span> <span class="o">=</span> <span class="n">dgroups</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="c1"># change model names in the column 'model'</span>
    <span class="n">eval_df_t</span><span class="p">[</span><span class="s1">'model'</span><span class="p">]</span> <span class="o">=</span> <span class="n">eval_df_t</span><span class="p">[</span><span class="s1">'model'</span><span class="p">]</span> <span class="o">+</span> <span class="s1">'-{}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> 
    <span class="n">eval_df_all</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">eval_df_all</span><span class="p">,</span> <span class="n">eval_df_t</span><span class="p">])</span>

<span class="c1"># sort new dataframe and display 10 best models</span>
<span class="n">eval_df_all</span> <span class="o">=</span> <span class="n">eval_df_all</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">'model'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">eval_df_all</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>22
</pre></div>


<p>We have trained 22 models in total.</p>
<p>Now, I are going to select winners.</p>
<p><strong>The best cross-validated score</strong></p>
<div class="highlight"><pre><span></span><span class="n">eval_df_all</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">'CV_score'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV_score</th>
      <th>f1_macro</th>
      <th>accuracy</th>
    </tr>
    <tr>
      <th>model</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>rf-pm</th>
      <td>0.787072</td>
      <td>0.410081</td>
      <td>0.789474</td>
    </tr>
  </tbody>
</table>
</div>

<p>The plain Random Forest model had the best cros-validated score. The default holdout score was accuracy. Practically, all models in the plain models group, were predicting the majority class only.</p>
<p><strong>The best accuracy score on the test set</strong></p>
<div class="highlight"><pre><span></span><span class="n">eval_df_all</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">'accuracy'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV_score</th>
      <th>f1_macro</th>
      <th>accuracy</th>
    </tr>
    <tr>
      <th>model</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>rf-f1</th>
      <td>0.405695</td>
      <td>0.473797</td>
      <td>0.824561</td>
    </tr>
  </tbody>
</table>
</div>

<p>The accuracy score is not the best option for multi-class classification tasks. The Random Forest model with f1-macro score, as the holdout score, was the best in this category.</p>
<p>And finally, I have a <strong>winning</strong> model</p>
<p><strong>The best f1_macro score on the test set - WINNER</strong></p>
<p>Let's list 10 best models evaluated with f1-macro score</p>
<div class="highlight"><pre><span></span><span class="n">eval_df_all</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">'f1_macro'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CV_score</th>
      <th>f1_macro</th>
      <th>accuracy</th>
    </tr>
    <tr>
      <th>model</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>rf-cs</th>
      <td>0.438159</td>
      <td>0.556052</td>
      <td>0.754386</td>
    </tr>
    <tr>
      <th>gb-sm</th>
      <td>0.439936</td>
      <td>0.522429</td>
      <td>0.736842</td>
    </tr>
    <tr>
      <th>gb-f1</th>
      <td>0.427936</td>
      <td>0.513450</td>
      <td>0.771930</td>
    </tr>
    <tr>
      <th>rf-sm</th>
      <td>0.458407</td>
      <td>0.511111</td>
      <td>0.719298</td>
    </tr>
    <tr>
      <th>ab-f1</th>
      <td>0.395413</td>
      <td>0.505135</td>
      <td>0.789474</td>
    </tr>
    <tr>
      <th>rf-f1</th>
      <td>0.405695</td>
      <td>0.473797</td>
      <td>0.824561</td>
    </tr>
    <tr>
      <th>l2-cs</th>
      <td>0.414028</td>
      <td>0.455797</td>
      <td>0.631579</td>
    </tr>
    <tr>
      <th>ab-sm</th>
      <td>0.415525</td>
      <td>0.437384</td>
      <td>0.640351</td>
    </tr>
    <tr>
      <th>l2-sm</th>
      <td>0.408130</td>
      <td>0.434748</td>
      <td>0.622807</td>
    </tr>
    <tr>
      <th>svm-cs</th>
      <td>0.401227</td>
      <td>0.432931</td>
      <td>0.728070</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
<li>
<p>Out of 22 models, the winner (the best f1_macro score) is Random Forest with  cost-sensitive learning.</p>
</li>
<li>
<p>The best 6 models are tree ensembles.</p>
</li>
<li>
<p>As expected, Top 10 is ocupied by models for which I used one of the techniques for handling imbalanced classes.</p>
</li>
</ul>
<p>Let's check the winner's confusion matrix and classification report for additional analysis.</p>
<div class="highlight"><pre><span></span><span class="c1"># Plot confusion matrix heatmaps without and with normalization</span>
<span class="n">plot_conf_mat_w_and_wo_norm</span><span class="p">(</span><span class="n">fitted_models_cs</span><span class="p">,</span> <span class="s1">'rf'</span><span class="p">,</span> <span class="s1">'YlOrRd'</span><span class="p">)</span>             
</pre></div>


<p><img alt="png" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/output_330_0.png"></p>
<div class="highlight"><pre><span></span><span class="c1"># Display classification report and confusion matrix</span>
<span class="n">class_rep_cm</span><span class="p">(</span><span class="n">fitted_models_cs</span><span class="p">,</span> <span class="s1">'rf'</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>     Random Forest
     =============
</pre></div>


<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted Labels</th>
      <th>no_diabetes</th>
      <th>pre_diabetes</th>
      <th>diabetes</th>
    </tr>
    <tr>
      <th>True Labels</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_diabetes</th>
      <td>73</td>
      <td>3</td>
      <td>12</td>
    </tr>
    <tr>
      <th>pre_diabetes</th>
      <td>4</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>diabetes</th>
      <td>7</td>
      <td>1</td>
      <td>11</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

 no_diabetes       0.87      0.83      0.85        88
pre_diabetes       0.33      0.29      0.31         7
    diabetes       0.46      0.58      0.51        19

   micro avg       0.75      0.75      0.75       114
   macro avg       0.55      0.56      0.56       114
weighted avg       0.77      0.75      0.76       114
</pre></div>


<p>As I discussed earlier, this model does OK on the majority class (recall=0.83) and not so bad on the diabetes class (recal=0.58). It would need some improvement on the least populated class, <code>pre_diabetes</code> (recall=0.29).</p>
<h2>Insights &amp; Analysis</h2>
<h3>Key Findings and Results</h3>
<p>Our dataset was relatively small and imbalanced and I had to employ several techniques for handling imbalanced classes:</p>
<ol>
<li>
<p>f1 macro averaged score for performance metric</p>
</li>
<li>
<p>cost-sensitive learning (penalize algorithms)</p>
</li>
<li>
<p>SMOTE - Synthetic Minority Over-sampling Technique</p>
</li>
</ol>
<p>We have used six machine learning algorithms: $L_1$ and $L_2$ regularized Logistic Regressions, SVM and three tree ensembles, Random Forest, Gradient Boost and AdaBoost.</p>
<p>In total, I have trained 22 models.</p>
<ul>
<li>
<p>Plain models, without any of the above listed techniques, did prety bad with predicting minority classes. They mostly predicted the majority class. Because of that, their accuracy score was high, but f1-macro score was low. As expected, tree ensembles models, were performed slightly better.</p>
</li>
<li>
<p>All three techniques listed above, made a positive difference. Again, tree ensemble models produced better performance.</p>
</li>
<li>
<p>I could not find one single health condition that could alone increase the risk of being diagnosed with type 2 diabetes.</p>
</li>
<li>
<p>It looks that they are working differently for different people.</p>
</li>
<li>
<p>From my limited sample, I could conclude that the most contributing factors were age, cholesterol ratio and waist cirumference.</p>
</li>
</ul>
<p><strong>Winning model</strong></p>
<p>The Random Forest model with cost-sensitive learning have produced the best performance:</p>
<ul>
<li>
<p>prety good on the majority class - recall=0.83</p>
</li>
<li>
<p>on the diabetes class not so bad - recall=0.58</p>
</li>
<li>
<p>not so good on the smallest, pre-diabetes, class - recall=0.29</p>
</li>
</ul>
<p><strong>It is interesting to note the following regarding predicting pre-diabetes:</strong></p>
<ul>
<li>
<p>there were only 7 labels in the test set</p>
</li>
<li>
<p>recall=0.29 means 2 successfully predicted labels and in addition to my winning model, only 3 models had this score: Gradient Boost, Random Forest and SVM all with SMOTE  </p>
</li>
<li>
<p>only 2 models succeded in 3 positive prediction, recall=0.43. Surprisingly, that was $L_2$-regularized Logistic Regression with SMOTE and cost-sensitive learning.</p>
</li>
</ul>
<h3>Possible Improvements and Expansions</h3>
<p>These are some of possible improvements/expansions for this project:</p>
<ul>
<li>
<p>Acquire a bigger dataset so that the smallest class will have more than 7 points in the test set. And maybe, a bigger sample could produce different relationships among the features.</p>
</li>
<li>
<p>Additional hyperparameters tuning on few best performing models</p>
</li>
<li>
<p>Perform additional feature selection and dimensionality reduction (PCA)</p>
</li>
</ul>
<h2>References</h2>
<ul>
<li>
<p><a href="https://stackoverflow.com/questions/50245684/using-smote-with-gridsearchcv-in-scikit-learn">Using Smote with Gridsearchcv in Scikit-learn</a></p>
</li>
<li>
<p><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics">Model evaluation: quantifying the quality of predictions</a></p>
</li>
<li>
<p><a href="https://www.kaggle.com/eikedehling/exploring-class-imbalance-resampling-and-weights">Exploring class imbalance, resampling and weights</a></p>
</li>
<li>
<p><a href="https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/">Multiclass classification using scikit-learn</a></p>
</li>
<li>
<p><a href="https://www.ritchieng.com/machine-learning-evaluate-classification-model/">Evaluating a Classification Model</a></p>
</li>
<li>
<p><a href="https://elitedatascience.com/imbalanced-classes">How to Handle Imbalanced Classes in Machine Learning</a></p>
</li>
<li>
<p><a href="https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18">Dealing with Imbalanced Data</a></p>
</li>
<li>
<p><a href="https://towardsdatascience.com/a-deep-dive-into-imbalanced-data-over-sampling-f1167ed74b5">A Deep Dive Into Imbalanced Data: Over-Sampling</a></p>
</li>
<li>
<p><a href="https://www.webmd.com/diabetes/guide/glycated-hemoglobin-test-hba1c?print=true">Hemoglobin A1c (HbA1c) Test for Diabetes</a></p>
</li>
<li>
<p><a href="https://www.mayoclinic.org/diseases-conditions/type-2-diabetes/diagnosis-treatment/drc-20351199">Type 2 diabetes</a></p>
</li>
<li>
<p><a href="https://www.health.harvard.edu/blog/rethinking-a1c-goals-for-type-2-diabetes-2018032613452">Rethinking A1c goals for type 2 diabetes</a></p>
</li>
<li>
<p><a href="https://www.cdc.gov/healthyweight/assessing/bmi/adult_bmi/index.html#Interpreted">About Adult BMI</a></p>
</li>
<li>
<p><a href="https://www.medicalnewstoday.com/articles/319439.php">Why is the hip-waist ratio important?</a></p>
</li>
<li>
<p><a href="https://universityhealthnews.com/daily/heart-health/cholesterol-ratio-more-important-than-total-cholesterol-or-ldl-cholesterol/">What Is Cholesterol Ratio?</a></p>
</li>
<li>
<p><a href="https://www.healthline.com/health/blood-pressure-chart">Hypertension Chart - Understanding Your Blood Pressure</a></p>
</li>
<li>
<p><a href="https://www.webmd.com/diabetes/news/20050310/waist-size-predicts-diabetes-risk?print=true">Waist Size Predicts Diabetes Risk</a></p>
</li>
</ul>
            <div class="hr"></div>
            <a href="http://localhost:8000/posts/2019/07/Machine%20Learning,%20July%202019,%20Risk%20prediction/#" class="go-top" style="display: inline;">Go Top</a>
<div class="comments">
    <div id="disqus_thread"><iframe id="dsq-app3517" name="dsq-app3517" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/saved_resource.html" style="width: 1px !important; min-width: 100% !important; border: none !important; overflow: hidden !important; height: 444px !important;" horizontalscrolling="no" verticalscrolling="no"></iframe><iframe id="indicator-north" name="indicator-north" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" style="width: 894px !important; border: none !important; overflow: hidden !important; top: 0px !important; min-width: 894px !important; max-width: 894px !important; position: fixed !important; z-index: 2147483646 !important; height: 0px !important; min-height: 0px !important; max-height: 0px !important; display: none !important;" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/saved_resource(1).html"></iframe><iframe id="indicator-south" name="indicator-south" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" style="width: 894px !important; border: none !important; overflow: hidden !important; bottom: 0px !important; min-width: 894px !important; max-width: 894px !important; position: fixed !important; z-index: 2147483646 !important; height: 0px !important; min-height: 0px !important; max-height: 0px !important; display: none !important;" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/saved_resource(2).html"></iframe></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = "leafyleap-2"; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
</div><footer class="footer">
    <p>© Mohcine Madkour –
        Built with <a href="https://github.com/PurePelicanTheme/pure">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
</div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>

<iframe style="display: none;" src="./Type 2 Diabetes - Risk Predictions __ Mohcine Madkour __ Big Data Architectures and more_files/saved_resource(3).html"></iframe></body></html>